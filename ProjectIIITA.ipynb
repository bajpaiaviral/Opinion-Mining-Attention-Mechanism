{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A5n3XB4HTGLX",
    "outputId": "6fb432de-dfb1-4e23-eebb-0afb29a8c4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEmLAv9_HUEu"
   },
   "source": [
    "# Loading And Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ItzpP4m2MJ6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MZtIG-u2N5A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B3Wb_idr2PeH",
    "outputId": "45c7a98f-6ead-4504-fc80-72c12e78e400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YPTlQ2xi2Q8H",
    "outputId": "d57a0abb-af87-4ccc-f650-19391a5b972b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=0\n",
    "tweets=[]\n",
    "id=[]\n",
    "y=[]\n",
    "z=[]\n",
    "words=[]\n",
    "max_length=50\n",
    "with open('/content/drive/My Drive/hinglish_data/train_14k_split_conll.txt','r') as file: \n",
    "  for line in file: \n",
    "    if len(line.split())==3:\n",
    "      id.append(line.split()[1])\n",
    "      y.append(line.split()[2])\n",
    "      tweets.append(z)\n",
    "      z=[]\n",
    "    elif len(line.split())==2:\n",
    "      z.append(line.split()[0])  \n",
    "      words.append(line.split()[0])\n",
    "    # if counter>=150:\n",
    "    #   break  \n",
    "    for word in line.split(): \n",
    "      #print(word)\n",
    "      a=1\n",
    "      #counter+=1\n",
    "tweets.append(z)\n",
    "tweets.pop(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSdYGXUU2TcM"
   },
   "outputs": [],
   "source": [
    "tweets2=[]\n",
    "count=1\n",
    "for sent in tweets:\n",
    "  i=0\n",
    "  z=[]\n",
    "  while i<len(sent):\n",
    "    #print(sent[i])\n",
    "    if(sent[i]=='https'):\n",
    "      z.append(\"\")\n",
    "      i+=6\n",
    "    elif(sent[i]=='@'):\n",
    "      z.append(\"\")\n",
    "      i+=1\n",
    "    elif sent[i]=='...' or sent[i]=='..' or sent[i]=='....' or sent[i]=='‚Ä¶':\n",
    "      z.append('.')\n",
    "    elif not sent[i]=='_':\n",
    "      z.append(sent[i])  \n",
    "    i+=1\n",
    "    count+=1\n",
    "    #print(z) \n",
    "  tweets2.append(z)   \n",
    "    #if(count>=300):\n",
    "    #  break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "JhdIWO4R2Wi-",
    "outputId": "79fc0379-e614-400a-d9bd-f8f701d54b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid Asi takhle vypad √° m ≈Ø j life .', ' Haan yaar neha üòîüòî kab karega woh post üò≠ Usne na sach mein photoshoot karna chahiye phir woh post karega . ', ' television media congress ke liye nhi h . Ye toh aapko pata chal hi gya hoga . Achha hoga ki Congress ke . ', '  All India me nrc lagu kare w Kashmir se dhara 370ko khatam kare ham Indian ko apse yahi umid hai', ' who   Pagal hai kya ? They aren ‚Äô t real issues Mandir is important Hindu khatre mei jo hai !']\n"
     ]
    }
   ],
   "source": [
    "temp2=[(\" \").join(i for i in tweets2[j]) for j in range(0,14000)]\n",
    "print(temp2[:5])\n",
    "def deSymbolify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        # u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text)\n",
    "    SMILEYS = {\n",
    "        \":‚Äë)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‚ÄëD\":\"smiley\",\n",
    "        \"8‚ÄëD\":\"smiley\",\n",
    "        \"x‚ÄëD\":\"smiley\",\n",
    "        \"X‚ÄëD\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‚Äë(\":\"sad\",\n",
    "        \":‚Äëc\":\"sad\",\n",
    "        \":‚Äë<\":\"sad\",\n",
    "        \":‚Äë[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‚Äë(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‚ÄëP\":\"playful\",\n",
    "        \"X‚ÄëP\":\"playful\",\n",
    "        \"x‚Äëp\":\"playful\",\n",
    "        \":‚Äëp\":\"playful\",\n",
    "        \":‚Äë√û\":\"playful\",\n",
    "        \":‚Äë√æ\":\"playful\",\n",
    "        \":‚Äëb\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":√û\":\"playful\",\n",
    "        \":√æ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "    text = text.replace('\\x92',\"'\")\n",
    "\n",
    "    words = text.split()\n",
    "    reformed = [SMILEYS[word] if word in SMILEYS else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp3=[]\n",
    "for i in temp2:\n",
    "  cleanstring=(clean_text(i))\n",
    "  cleanstring=deSymbolify(cleanstring)\n",
    "  temp3.append(cleanstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWXph7Pl3Qvp"
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in temp3:\n",
    "  li=i.split(\" \")\n",
    "  indexes=set()\n",
    "  for index in range(len(li)-1):\n",
    "    if(li[index]=='user'):\n",
    "      indexes.add(index)\n",
    "      \n",
    "  answer=[]\n",
    "  for index in range(len(li)):\n",
    "    if(index not in indexes):\n",
    "      answer.append(li[index])\n",
    "      \n",
    "  \n",
    "  temp3[c]=\" \".join(answer)\n",
    " \n",
    "  c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE03r-2S4fsq"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['tweets']=temp3\n",
    "df['sentiment']=y\n",
    "df['sentiment'].replace(['neutral','positive','negative'],[0,1,2],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5Kg7w6266er"
   },
   "outputs": [],
   "source": [
    "counter=0;zz=[];id=[];y=[];z=[]\n",
    "\n",
    "with open('/content/drive/My Drive/hinglish_data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='UTF') as file: \n",
    "  for line in file: \n",
    "    if len(line.split())==2:\n",
    "      if line.split()[0]=='meta':\n",
    "        zz.append(z)\n",
    "        id.append(line.split()[1])\n",
    "        z=[]\n",
    "      else:\n",
    "        z.append(line.split()[0])\n",
    "zz.append(z)\n",
    "zz.pop(0)\n",
    "twee=[(\" \").join(i for i in zz[j]) for j in range(0,len(zz))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA00QG3--ooe"
   },
   "outputs": [],
   "source": [
    "test_tweets=[]\n",
    "for i in twee:\n",
    "  test_tweets.append(clean_text(deSymbolify((i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bn_3KxxI69H7"
   },
   "outputs": [],
   "source": [
    "testlabels=pd.read_csv('/content/drive/My Drive/hinglish_data/testlabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WHEv9cC-I6b"
   },
   "outputs": [],
   "source": [
    "testlabels['Sentiment'].replace(['neutral','positive','negative'],[0,1,2],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9vhguZs-XCW"
   },
   "source": [
    "# Multi Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kTGC_jC8-PIi",
    "outputId": "2dbca308-22f4-4652-f3e2-68cda3f9746f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "import keras\n",
    "from keras import regularizers,layers\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYnwzufd-dOn"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(df['sentiment'])\n",
    "X_train=np.array(df['tweets'])\n",
    "X_test=test_tweets\n",
    "y_test=np.array(testlabels['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbPpP8s7-e1a"
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Q0iPmWub-xlI",
    "outputId": "d73d8313-bfa2-4ca9-cf06-c347c443d61f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17000,)\n",
      "(14000,)\n",
      "3\n",
      "16853\n",
      "['n', 'e', ' ', '√°', 'v', 'i', 's', 't', 'b', 'o', 'l', 'z', 'k', 'm', 'u', 'a', 'ƒõ', 'd', 'j', 'c', 'h', 'y', 'p', '≈Ø', 'f', '.', 'r', '_', 'g', 'w', '3', '7', '0', '?', '‚Äô', '!', '-', '#', 'x', 'q', \"'\", '‚Ä¶', '~', '1', '%', '*', '4', ')', '2', '5', '+', '‚Äì', '&', '9', '‚Ä¢', '(', '@', 'T', 'O', 'P', '‚ï≠', '‚ïÆ', '„Ä£', '¬∫', 'Œ¥', '6', '\"', '/', '8', 'K', '‚Äò', '‚Äú', '‚Äù', '>', '‡§Æ', '‡•á', '‡§∞', '‡§ö', '‡•ç', '‡§™', '‡§æ', '‡§£', '‡§ï', '‡§∂', '¬£', '‚Ç¨', '$', '‡§®', '‡§ø', '‡§ß', '‡§Ø', '‡•ã', '‡§ú', '‚Äî', '‚òÖ', '{', '}', '‡∞Æ', '‡∞®', '‡∞ï', '‡∞ø', '‡∞¶', '‡±ç', '‡∞ß', '‡∞§', '‡±Å', '‡∞∞', '‡∞æ', '‡∞¨', '‡±ã', '‡∞Ø', '‡±á', '‡∞á', '‡∞∏', '‡∞Ö', '‡∞≤', '‡∞â', '‡∞Ç', '‡∞°', '‡∞™', '‡∞∑', '‡±Ç', '‡∞µ', '‡∞ö', '‡§§', '‡§≤', '‡§Ç', '‡§¶', '‡§µ', '‡§∑', '‡§≠', '·∫°', '·∫Ω', '√™', '√©', '‡§π', '‡•à', '‡§∏', '‡•Ç', '‡•Ä', '‡§•', 'I', 'Œº', '[', ']', '‡πÄ', '‡∏û', '‡∏•', '‡∏á', '‡∏°', '‡∏µ', '‡πÅ', '‡πâ', '‡∏ß', '‡∏ö', '‡∏ô', '‡∏ï', '‡∏ü', '‡∏≠', '‡∏£', '‡πå', '‡∏™', '‡∏¥', '‡πà', '‡∏ó', '‡∏±', '‡∏´', '‡∏î', '‡∏Ñ', '‡∏∞', '‡•Å', '¬•', '√º', 'ÔΩì', 'ÔΩï', 'ÔΩé', 'ÔΩÖ', 'ÔΩî', '=', ';', '‡•å', '|', '<', 'ÿ¨', 'ŸÖ', 'ÿπ', 'ÿ©', 'ÿß', 'ŸÑ', 'Ÿà', 'ÿØ', 'ƒü', '√ß', 'ƒ±', 'Ãá', '√∂', '√≤', '‚ÑÖ', '√≠', '‚ñ∫', '‡§ó', '‡§ë', '‡§´', '‡§ü', '‡§¨', 'ƒÅ', '√†', '≈°', 'N', 'E', 'A', 'ƒì', 'ÿ™', '€å', 'ÿ±', 'ÿµ', '‚ô°', '¬∞', 'ùÑû', '‚Éë', '`', '¬¥', '‡§è', '„Ö†', '√•', '√§', '√Ω', 'ƒç', '≈ô', '√£', '√¥', '·ªâ', '∆∞', '·ªù', '‡¶à', '‡¶¶', '‡¶Æ', '‡ßã', '‡¶¨', '‡¶æ', '‡¶∞', '‡¶ï', '‡§Ö', '‡§°', '‡§º', '‡§ñ', '„ÅØ', '„ÉÄ', '„Éí', '„Éß', '„É≥', '„ÅÆ', 'Ë™ï', 'Áîü', 'Êó•', 'ÂΩì', 'Èôê', 'ÂÆö', '„Åß', '„ÄÅ', '„Ç§', '„Éô', '„Éà', '„Çπ', '„ÉÜ', '„Éº', '„Ç∏', '„Åå', '„Å§', '„ÅÑ', '„Å´', 'Èñã', 'Êîæ', '„Åô', '„Åπ', '„Å¶', '„Éë', '„Ç∫', '„É´', '„Çí', '„ÇØ', '„É™', '„Ç¢', '„Çã', '„Å®', '„É†', '„Éì', '„Åæ', '‡πÑ', '‡πÉ', '‡∏à', '‡∏≤', '‡∏ñ', '‡∏ò', '‡∏ì', '‡∏¢', '‡∏Ç', '‡∏Å', '‡∏∑', '‡§Ü', 'G', 'B', 'Z', 'S', 'L', 'F', '‡§î', '≈ü', 'Ÿä', 'ŸÇ', 'ƒá', '≈º', 'ƒô', '≈õ', '≈Ç', 'Ô∑ª', '^', '‡•§', '\\u200b', '‚áí', 'D', '‚†Ä', '‚Çπ', '‡•â', '€É', 'ƒÖ', '≈∫', '√≥', '‚Üë', '≈à', 'ùêπ', 'ùíæ', 'ùëî', 'ùíΩ', 'ùìâ', 'ùíª', 'ùëú', 'ùìá', 'ùìä', 'ùìà', '√Æ', '‚Äï', '¬°', '\\u200d', '·µí', ' ∞', '·µê', ' ∏', '·µç', '·µà', '·µò', '·µÉ', ' ≥', '·µâ', 'À¢', '·∂ú', '·µó', '·∂¶', '·µõ', '·µè', '√∫', '‡§Å', '‡§õ', '‡§ù', '‡§†', 'M', '‡§á', '‡§à', '√±', 'U', '¬≤', 'Áå´', '„Å≠', '„Åì', '„Éç', '„Ç≥', '\\u2066', '\\u2069', '‡®π', '‡©Å', '‡®£', '‡®§', '‡®∏', '‡©Ä', '‡®Ç', '‡®®', '‡®ï', '‡®∞', '‡©ã', '‡®ó', '‡©á', '‡®æ', '‡®´', '‡®ø', '√ó', '€í', '⁄∫', 'ÿ≥', '€Å', '¬∑', '¬Ø', '‚ñÇ', '„Å£', 'Àò', 'Ã©', 'ÿ¥', 'ÿÆ', 'Î©Ä', 'Ìã∞', 'Î∑∞', 'ÏôÄ', 'Ìï®', 'Íªò', 'Ï¶ê', 'Í∏∞', 'Îäî', 'Î®∏', 'Ïä§', 'ÌÑ∞', 'Ïã§', 'Ìô©', 'ÏÉù', 'Ï§ë', 'Í≥Ñ', 'Îßå', 'Îã§', 'Î¶Ω', 'Îãà', '–º', '–æ', '–¥', '–∫', '–∞', '—Ä', '—Ç', '’¨', '…©', '…£', '—î', '…≤', 'ƒë', '·∫ø', '·ªì', '·ªß', '·ªÅ', '·ªë', '‡•É', '‡§É', 'Ìïú', 'Íµ≠', 'ÎåÑ', 'Îü¨', 'Ïãú', 'ÌåÄ', 'ÏïΩ', 'Ïûê', 'Ï†ú', 'Ïùº', 'ÏùÄ', 'Ìñâ', 'ƒ´', '√¨', 'C', 'Íïä', 'ùêÇ', 'ùê°', 'ùêö', 'ùê©', 'ùê≠', 'ùêû', 'ùê´', 'ùüî', 'ùüè', 'ùüê', 'ùêâ', 'ùêÆ', 'ùêß', 'ùêñ', 'ùê¢', 'ùê¨', 'Ô∏é', '‡∏ú', '‡∏õ', '‡∏∂', '‡∏∏', '‡∏π', '√∞', '√ø', 'Àú', '√¢', '¬¶', '¬®', '¬æ', '¬§', '¬∏', '‚Äπ', '¬π', '\\x90', '≈ì', '¬ø', '\\xad', 'ÀÜ', '‚Ä°', '‚Ä†', '¬ß', '\\x81', '¬µ', '\\x9d', '¬≥', '‚Äö', '\\x8f', '‚Ä∞', '√Ø', '≈æ', '¬∂', '‚Äû', '\\x8d', '¬ª', '‚Ä∫', '∆í', '¬Ω', '¬™', '√π', '√∏', '¬±', '¬¨', '√´', '¬¢', '¬º', '¬´', '√ª', '√®']\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(len(np.unique(y)))\n",
    "print(len(np.unique(X)))\n",
    "v=[]\n",
    "for i in X:\n",
    "  for j in i:\n",
    "    if j not in v:\n",
    "      v.append(j)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PRUUGKv-0IL"
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "dc=dict()\n",
    "for i in X:\n",
    "  l=i.split(\" \")\n",
    "  for k in l:\n",
    "    if (k!=\" \" and k not in dc.keys()):\n",
    "      dc[k]=1\n",
    "    else:\n",
    "      dc[k]=dc[k]+1\n",
    "l=[]\n",
    "\n",
    "cnt=0\n",
    "word_to_id=dict()\n",
    "for i in X:\n",
    "  l=i.split(\" \")\n",
    "  for k in l:\n",
    "    if (k!=\" \" and k not in word_to_id.keys() and dc[k]>5):\n",
    "      word_to_id[k]=cnt\n",
    "      cnt=cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PhETX1oI-2ci",
    "outputId": "ebb68458-b322-4d3b-8e89-76ee87aa12de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'√°': 0, 'lost': 1, 'j': 2, 'a': 3, 'asi': 4, 'm': 5, 'life': 6, '.': 7, 'haan': 8, 'yaar': 9, 'pensive_face': 10, 'kab': 11, 'karega': 12, 'woh': 13, 'post': 14, 'loudly_crying_face': 15, 'usne': 16, 'na': 17, 'sach': 18, 'mein': 19, 'karna': 20, 'chahiye': 21, 'phir': 22, 'media': 23, 'congress': 24, 'ke': 25, 'liye': 26, 'nhi': 27, 'h': 28, 'ye': 29, 'toh': 30, 'aapko': 31, 'pata': 32, 'chal': 33, 'hi': 34, 'gya': 35, 'hoga': 36, 'achha': 37, 'ki': 38, 'all': 39, 'india': 40, 'me': 41, 'nrc': 42, 'lagu': 43, 'kare': 44, 'w': 45, 'kashmir': 46, 'se': 47, 'dhara': 48, 'khatam': 49, 'ham': 50, 'indian': 51, 'ko': 52, 'apse': 53, 'yahi': 54, 'umid': 55, 'hai': 56, 'who': 57, 'pagal': 58, 'kya': 59, '?': 60, 'they': 61, '‚Äô': 62, 't': 63, 'real': 64, 'issues': 65, 'mandir': 66, 'is': 67, 'important': 68, 'hindu': 69, 'khatre': 70, 'mei': 71, 'jo': 72, '!': 73, 'jeet': 74, 'dher': 75, 'sari': 76, 'subh': 77, 'modi': 78, 'ji': 79, 'asha': 80, 'karta': 81, 'hu': 82, 'desh': 83, 'janta': 84, 'ne': 85, 'khas': 86, 'kar': 87, 'bihar': 88, 'but': 89, 'topi': 90, 'walay': 91, 'babu': 92, 'tau': 93, 'new': 94, 'job': 95, 'chutti': 96, 'gi': 97, 'walon': 98, 'bhi': 99, 'maza': 100, 'ata': 101, 'muslim': 102, 'debate': 103, 'karne': 104, 'tere': 105, 'pakistan': 106, 'chutiye': 107, 'khelne': 108, 'de': 109, 'unhe': 110, 'bc': 111, 'tu': 112, 'nahi': 113, 'hehe': 114, 'i': 115, 'saw': 116, 'that': 117, 'coming': 118, 'and': 119, 'it': 120, 's': 121, 'actually': 122, 'from': 123, 'someone': 124, 'shaadi': 125, 'star-struck': 126, 'can': 127, 'you': 128, 'answer': 129, 'votes': 130, 'on': 131, 'each': 132, 'seat': 133, 'one': 134, 'vote': 135, 'matters': 136, '#': 137, 'aur': 138, 'apne': 139, 'allah': 140, 'pak': 141, 'os': 142, 'k': 143, 'dil': 144, 'ka': 145, 'kbi': 146, 'off': 147, 'nai': 148, 'krta': 149, 'sb': 150, 'ache': 151, 'aor': 152, 'dua': 153, 'ma': 154, 'ho': 155, 'bahut': 156, 'faisla': 157, 'liya': 158, 'ummeed': 159, 'hoon': 160, 'sabhi': 161, 'party': 162, 'rt': 163, 'safar': 164, 'mera': 165, 'meri': 166, 'bina': 167, 'aye': 168, 'mushkil': 169, 'heart_with_arrow': 170, 'see-no-evil_monkey': 171, 'love': 172, 'u': 173, 'red_heart': 174, '!!': 175, 'pm': 176, 'bnne': 177, 'ummid': 178, 'aapke': 179, 'isa': 180, 'bhut': 181, 'si': 182, 'we': 183, 'face': 184, 'together': 185, 'my': 186, 'money': 187, 'the': 188, 'both': 189, 'of': 190, 'us': 191, 'still': 192, 'waah': 193, 'kisi': 194, 'khub': 195, 'likha': 196, 'khud': 197, 'jala': 198, 'jaha': 199, 'roshan': 200, 'fir': 201, 'kiya': 202, 'tha': 203, 'aapne': 204, 'aap': 205, 'ba': 206, 'abe': 207, 'tum': 208, 'flop': 209, 'logo': 210, 'chutiya': 211, 'log': 212, 'ja': 213, 'bad': 214, '__': 215, 'very': 216, 'beautiful': 217, 'eyes': 218, 'shit': 219, 'fuck': 220, 'up': 221, 'haram': 222, 'khor': 223, 'ulta': 224, 'chor': 225, 'mulk': 226, 'bat': 227, 'or': 228, 'har': 229, 'army': 230, 'also': 231, 'for': 232, 'concert': 233, 'looks': 234, 'so': 235, 'smiling_face_with_heart-eyes': 236, 'see': 237, 'their': 238, 'instagram': 239, 'jai': 240, 'shree': 241, 'ram': 242, 'jarurat': 243, 'aandolan': 244, 'guruji': 245, 'to': 246, 'pls': 247, 'trust': 248, 'our': 249, 'chalu': 250, 'gaya': 251, 'bjp': 252, 'walo': 253, 'tumhe': 254, 'kuch': 255, 'chut': 256, 'mil': 257, 'raha': 258, 'ya': 259, 'milna': 260, 'q': 261, 'jinko': 262, 'pta': 263, 'wo': 264, 'bhej': 265, 'rahe': 266, 'neta': 267, 'phle': 268, 'f': 269, 'yhi': 270, 'rajya': 271, 'jisme': 272, 'sbhi': 273, 'sale': 274, 'harami': 275, 'itne': 276, 'maar': 277, 'rhe': 278, 'dekhna': 279, 'be': 280, 'maybe': 281, \"i'll\": 282, \"won't\": 283, 'feel': 284, 'thing': 285, 'duniya': 286, 'koi': 287, 'kabhi': 288, 'hota': 289, 'yar': 290, 'pyar': 291, 'kaho': 292, 'do': 293, 'p': 294, \"i'm\": 295, 'poor': 296, 'happy': 297, 'god': 298, 'with': 299, 'smiling_face_with_halo': 300, 'madam': 301, 'always': 302, 'miss': 303, 'as': 304, 'videsh': 305, 'mantri': 306, 'sharma': 307, 'episode': 308, 'nice': 309, 'sir': 310, 'maan': 311, 'gaye': 312, 'himmat': 313, 'wa': 314, 'birthday': 315, 'doctor': 316, 'sahab': 317, 'bouquet': 318, 'bhagwaan': 319, 'khush': 320, 'rkhe': 321, 'hme': 322, 'aise': 323, 'apni': 324, 'birthday_cake': 325, 'teri': 326, 'kimat': 327, 'gayi': 328, 'amit': 329, 'shah': 330, 'will': 331, 'soon': 332, 'resign': 333, 'ek': 334, 'bhe': 335, 'bhai': 336, 'wala': 337, 'sab': 338, 'ker': 339, 'yeh': 340, 'bahen': 341, '..‚Ä¶': 342, 'mamta': 343, 'banerjee': 344, 'jane': 345, 'tumko': 346, 'haar': 347, 'ghum': 348, 'bus': 349, 'b': 350, 'far': 351, 'better': 352, 'than': 353, 'govt': 354, 'if': 355, 'r': 356, 'economy': 357, 'share': 358, 'your': 359, 'chanda': 360, 'make': 361, 'parti': 362, 'ha': 363, 'should': 364, 'hope': 365, 'fr': 366, 'best': 367, '-': 368, 'd': 369, 'woo': 370, 'hoo': 371, 'cricket': 372, 'world': 373, 'cup': 374, 'starts': 375, 'today': 376, 'good': 377, 'luck': 378, 'in': 379, 'tournament': 380, 'st': 381, 'family': 382, 'keh': 383, 'ni': 384, 'cause': 385, 'fake': 386, 'naam': 387, 'sai': 388, 'n': 389, 'pics': 390, 'krti': 391, 'ghr': 392, 'confused_face': 393, 'mp': 394, 'dr': 395, 'manoj': 396, 'g': 397, 'subhkamnaye': 398, 'apna': 399, 'maare': 400, 'fucked': 401, 'hum': 402, '~': 403, 'back': 404, '100%': 405, 'again': 406, 'feels': 407, '*': 408, 'bheja': 409, 'apke': 410, 'gadkari': 411, 'badhai': 412, 'modiji': 413, 'raj': 414, '.....': 415, 'this': 416, 'has': 417, '40': 418, 'famous': 419, 'ads': 420, '1)': 421, 'which': 422, 'favourite': 423, 'an': 424, 'ad': 425, 'spot': 426, '2)': 427, 'anyone': 428, 'damn': 429, 'give': 430, 'lot': 431, 'credit': 432, 'how': 433, 'work': 434, 'goes': 435, 'making': 436, 'his': 437, 'then': 438, 'been': 439, 'at': 440, 'rahega': 441, 'lekin': 442, 'pehle': 443, '370': 444, 'hatao': 445, 'samne': 446, 'roti': 447, 'sahi': 448, 'thakur': 449, 'saab': 450, 'bolke': 451, '50': 452, 'saal': 453, 'im': 454, 'proud': 455, '!!!!': 456, 'green_heart': 457, 'congratulations': 458, '+': 459, 'muslims': 460, 'jab': 461, 'unki': 462, 'izzat': 463, 'hone': 464, 'hadd': 465, 'kamine': 466, 'pan': 467, \"'\": 468, 'jail': 469, 'order': 470, 'railway': 471, 'pa': 472, \"it's\": 473, 'not': 474, 'vishwas': 475, 'guys': 476, 'want': 477, 'its': 478, 'dont': 479, 'even': 480, 'comes': 481, 'tel': 482, 'dekhe': 483, 'hoge': 484, 'bas': 485, 'jitana': 486, 'hey': 487, 'diye': 488, 'ap': 489, 'trha': 490, 'he': 491, 'olad': 492, 'pakistani': 493, 'sy': 494, 'krty': 495, 'fact': 496, 'check': 497, 'shri': 498, 'out': 499, 'vande': 500, 'mataram': 501, 'day': 502, 'wishes': 503, 'eid': 504, 'mubarak': 505, 'national': 506, 'general': 507, 'madad': 508, 'rss': 509, 'pas': 510, 'rakho': 511, 'kaafi': 512, 'dushman': 513, 'nahin': 514, 'hain': 515, 'kyun': 516, 'karen': 517, '‚Äì': 518, 'thinking': 519, 'about': 520, 'something': 521, 'inke': 522, 'pass': 523, 'ab': 524, 'bacha': 525, 'pasand': 526, 'ban': 527, 'no': 528, 'find': 529, 'perfect': 530, 'match': 531, 'zaror': 532, 'dukh': 533, 'phr': 534, 'khel': 535, 'talent': 536, 'like': 537, 'pleading_face': 538, 'here': 539, 'picture': 540, 'her': 541, 'thank': 542, 'morning': 543, 'mean': 544, 'problem': 545, 'was': 546, 'being': 547, 'vivek': 548, 'main': 549, 'attention': 550, 'bheek': 551, 'mang': 552, 'khoon': 553, 'laga': 554, '.‚Ä¶': 555, 'dearest': 556, 'friend': 557, 'maa': 558, 'bless': 559, 'lots': 560, 'month': 561, 'ramadan': 562, 'positive': 563, 'heart': 564, 'prayers': 565, 'friends': 566, 'insha': 567, 'oye': 568, 'meh': 569, 'jcb': 570, 'khudai': 571, 'dekh': 572, 'aaya': 573, '&': 574, 'other': 575, 'sirf': 576, 'payi': 577, 'jaati': 578, 'thi': 579, 'iska': 580, 'leader': 581, 'sarkar': 582, 'phal': 583, 'vidhayak': 584, 'par': 585, 'apny': 586, 'karo': 587, '2': 588, 'jiski': 589, 'pe': 590, 'aaj': 591, 'alvida': 592, 'guzarish': 593, 'name': 594, 'pr': 595, 'pada': 596, 'ao': 597, 'nagar': 598, 'rajiv': 599, 'garden': 600, 'bank': 601, 'dekho': 602, 'jyda': 603, 'gyan': 604, 'democracy': 605, 'majak': 606, 'band': 607, 'kro': 608, 'thoda': 609, 'dete': 610, 'baat': 611, 'bolte': 612, 'khan': 613, 'raho': 614, 'pray': 615, 'coz': 616, 'know': 617, 'nafrat': 618, 'kis': 619, 'now': 620, 'too': 621, 'much': 622, 'sare': 623, 'tweet': 624, 'arsal': 625, 'support': 626, 'angry_face': 627, 'jiya': 628, 'man': 629, 'chai': 630, 'bana': 631, 'sakti': 632, 'liy': 633, 'salo': 634, 'case': 635, 'aata': 636, 'inko': 637, 'public': 638, 'paise': 639, 'secretofpositivity': 640, 'papaji': 641, 'dhan': 642, 'satguru': 643, 'tera': 644, 'aasra': 645, 'she': 646, 'bachi': 647, 'piece': 648, 'maal': 649, '(': 650, 'rehmat': 651, ')': 652, 'mat': 653, 'wese': 654, 'shakal': 655, 'mjhe': 656, 'aesa': 657, 'kch': 658, 'matter': 659, 'face_with_tongue': 660, 'every': 661, 'single': 662, 'amazing': 663, 'grinning_face_with_smiling_eyes': 664, 'jindabaad': 665, 'entertainment': 666, 'kaun': 667, 'dega': 668, 'performance': 669, 'sabse': 670, 'watch': 671, '9': 672, 'uski': 673, 'jaat': 674, 'musalman': 675, 'hoti': 676, 'ready': 677, 'hume': 678, 'yun': 679, 'paoge': 680, 'soul': 681, 'leke': 682, 'rha': 683, 'face_with_tears_of_joy': 684, 'gunde': 685, 'saale': 686, 'ac': 687, 'hall': 688, 'baith': 689, 'mt': 690, 'bhonk': 691, '@_': 692, 'yogendrayadav': 693, 'aukat': 694, 'c': 695, 'bengal': 696, 'tmc': 697, 'galat': 698, 'app': 699, 'bachane': 700, 'isse': 701, 'baj': 702, 'liverpool': 703, 'winning': 704, 'them': 705, 'league': 706, 'joke': 707, 'already': 708, 'had': 709, 'pretty': 710, 'girl': 711, 'smiling_cat_face_with_heart-eyes': 712, 'memories': 713, 'cards': 714, 'hahahahaha': 715, 'enjoy': 716, 'jan': 717, 'sharam': 718, 'haya': 719, 'liay': 720, 'kute': 721, 'sapna': 722, 'din': 723, 'kerna': 724, 'fan': 725, 'hote': 726, 'mai': 727, 'dear': 728, 'shahrukh': 729, 'hugging_face': 730, 'mere': 731, 'yah': 732, 'tho': 733, 'pahele': 734, 'may': 735, 'rahegi': 736, 'inshallah': 737, 'bharatthiseid': 738, 'bharat': 739, 'kiss_mark': 740, 'superb': 741, 'look': 742, 'handsome': 743, 'bohat': 744, 'nasha': 745, 'howa': 746, 'aj': 747, 'kal': 748, 'jao': 749, 'wrna': 750, 'mar': 751, 'bar': 752, 'baba': 753, '31': 754, '2019': 755, 'happiness': 756, 'om': 757, 'please': 758, 'jet': 759, 'employee': 760, 'bare': 761, 'kariye': 762, 'parivar': 763, 'wale': 764, 'pareshan': 765, 'property': 766, 'rakh': 767, 'kr': 768, 'wal': 769, 'waseem': 770, 'respected': 771, 'rahul': 772, 'great': 773, 'soch': 774, 'same': 775, 'did': 776, 'bet': 777, '??': 778, 'kinda': 779, 'sexy': 780, 'fit': 781, 'jay': 782, 'bajrang': 783, 'bali': 784, 'thora': 785, 'bt': 786, 'last': 787, 'hamare': 788, 'hamein': 789, 'kutte': 790, 'haaye': 791, 'entry': 792, 'aisi': 793, 'part': 794, \"today's\": 795, 'ind': 796, 'inse': 797, 'payega': 798, '12': 799, 'year': 800, 'old': 801, 'record': 802, 'watching': 803, 'badshah': 804, 'times': 805, 'biggest': 806, 'sorry': 807, 'health': 808, 'department': 809, 'hospital': 810, 'priyanka': 811, 'prakash': 812, 'dada': 813, 'unse': 814, 'kehna': 815, 'chahti': 816, 'hun': 817, 'baar': 818, 'v': 819, 'le': 820, 'easy': 821, 'movie': 822, 'ny': 823, 'nh': 824, 'election': 825, 'gali': 826, 'ak': 827, 'kaise': 828, 'hui': 829, 'voters': 830, 'bolne': 831, 'pmo': 832, 'unke': 833, 'raat': 834, 'lage': 835, 'salman': 836, 'heavy_multiplication_x': 837, 'fight': 838, 'scene': 839, 'photo': 840, 'by': 841, 'likhta': 842, 'hrami': 843, 'aulad': 844, 'hogya': 845, 'bs': 846, 'bhakt': 847, 'arey': 848, 'didi': 849, 'gyi': 850, 'fearful_face': 851, 'bechari': 852, '5': 853, 'tweets': 854, 'kuki': 855, 'chalo': 856, '1000': 857, 'followers': 858, 'mujhy': 859, 'krna': 860, 'warna': 861, 'ga': 862, 'muslmano': 863, 'jisko': 864, 'follow': 865, 'google': 866, 'when': 867, 're': 868, 'sick': 869, 'narendra': 870, 'damodar': 871, 'shubh': 872, 'apka': 873, 'e': 874, 'talented': 875, 'such': 876, 'person': 877, 'smiling_face_with_3_hearts': 878, 'continue': 879, 'lagana': 880, 'pyaar': 881, 'odisha': 882, 'thanks': 883, 'hamara': 884, 'folded_hands': 885, 'jagannath': 886, 'swami': 887, 'pubg': 888, 'khelta': 889, 'use': 890, 'lagata': 891, 'dm': 892, 'society': 893, 'accept': 894, 'namaskar': 895, 'tak': 896, 'street': 897, 'light': 898, 'saath': 899, 'deshdrohi': 900, 'dur': 901, 'bacho': 902, 'pari': 903, \"can't\": 904, 'say': 905, 'kharab': 906, 'ghatiya': 907, 'gareebo': 908, 'rah': 909, 'jiske': 910, 'paas': 911, 'zameen': 912, 'kisan': 913, 'samman': 914, 'yojna': 915, 'diya': 916, 'dene': 917, 'celebrate': 918, 'star': 919, 'tonight': 920, 'ynwa': 921, 'london': 922, 'join': 923, 'tomorrow': 924, 'opening': 925, 'night': 926, '\"': 927, 'knowledge': 928, 'gandu': 929, 'harne': 930, 'wali': 931, 'sub': 932, 'tarah': 933, 'bhau': 934, 'iss': 935, 'budget': 936, 'kaam': 937, 'direct': 938, 'set': 939, 'grinning_face_with_sweat': 940, 'dal': 941, 'kripya': 942, 'nd': 943, 'police': 944, 'loot': 945, 'o': 946, 'get': 947, 'just': 948, 'funny': 949, 'sense': 950, 'humour': 951, 'anyway': 952, 'those': 953, 'crazy': 954, 'kids': 955, 'ah': 956, 'cute': 957, 'these': 958, 'two': 959, 'grinning_squinting_face': 960, 'two_hearts': 961, 'pranam': 962, 'seen': 963, 'abhi': 964, 'voice': 965, 'where': 966, 'finally': 967, 'digital': 968, \"''\": 969, 'baten': 970, 'journey': 971, 'less': 972, 'ask': 973, 'leave': 974, 'reason': 975, 'hello': 976, 'what': 977, 'side': 978, 'west': 979, 'through': 980, 'country': 981, 'acharya': 982, 'di': 983, 'gadhe': 984, 'bol': 985, 'sakte': 986, 'mal': 987, 'ajay': 988, 'didn': 989, 'have': 990, 'few': 991, 'movies': 992, 'point': 993, 'taken': 994, 'doing': 995, 'wife': 996, 'lagate': 997, 'abi': 998, 'tk': 999, 'kutty': 1000, 'namak': 1001, 'video': 1002, 'viral': 1003, 'besharam': 1004, 'kitna': 1005, 'tumara': 1006, 'anti': 1007, 'dukan': 1008, 'hogaya': 1009, 'andh': 1010, 'bhakts': 1011, 'are': 1012, 'increase': 1013, 'rate': 1014, 'air': 1015, 'home': 1016, 'hell': 1017, 'yeah': 1018, 'does': 1019, 'chunaw': 1020, 'tension': 1021, 'dusra': 1022, 'sant': 1023, \"she's\": 1024, 'kaha': 1025, 'purchase': 1026, 'any': 1027, 'land': 1028, 'hamari': 1029, 'raksha': 1030, 'bhagvan': 1031, 'sakta': 1032, 'jahan': 1033, 'system': 1034, 'hon': 1035, 'prime': 1036, 'minister': 1037, 'victory': 1038, 'ur': 1039, 'bhumat': 1040, 'victory_hand': 1041, 'bewakoof': 1042, 'aadmi': 1043, 'isne': 1044, 'wtf': 1045, 'fucking': 1046, 'annoying': 1047, 'imagine': 1048, 'baad': 1049, 'iqbal': 1050, 'advance': 1051, 'msg': 1052, 'send': 1053, 'krne': 1054, 'cat_face_with_tears_of_joy': 1055, 'taraf': 1056, 'aane': 1057, 'saza': 1058, 'badle': 1059, 'l': 1060, 'kat': 1061, 'shuru': 1062, 'hua': 1063, 'uclfinal': 1064, 'delhi': 1065, 'upar': 1066, 'garmi': 1067, 'alag': 1068, 'rakhi': 1069, 'gandhi': 1070, 'chowkidar': 1071, 'nara': 1072, 'bhari': 1073, 'pad': 1074, 'bail': 1075, 'badi': 1076, 'ise': 1077, 'sena': 1078, 'dono': 1079, 'kyu': 1080, 'bola': 1081, 'aisa': 1082, 'isko': 1083, 'gira': 1084, 'srk': 1085, 'charm': 1086, 'personality': 1087, 'swag': 1088, 'hero': 1089, 'role': 1090, 'sbse': 1091, 'jayda': 1092, 'aadat': 1093, 'gandi': 1094, 'ravan': 1095, 'bhagwan': 1096, 'samjhne': 1097, 'boht': 1098, 'bra': 1099, 'th': 1100, 'narendar': 1101, 'sabi': 1102, 'kaman': 1103, 'free': 1104, 'place': 1105, 'official': 1106, 'behalf': 1107, 'haha': 1108, 'kesy': 1109, 'yr': 1110, 'rhy': 1111, 'usi': 1112, 'galiya': 1113, 'gy': 1114, 'shame': 1115, 'yrr': 1116, 'agar': 1117, 'karke': 1118, 'pet': 1119, 'usme': 1120, 'burai': 1121, 'bohot': 1122, 'com': 1123, 'rail': 1124, 'tatkal': 1125, 'ticket': 1126, 'per': 1127, 'lagta': 1128, 'sarkari': 1129, 'honi': 1130, 'chayiye': 1131, 'y': 1132, 'lo': 1133, 'bnaye': 1134, 'class': 1135, 'excited': 1136, 'sam': 1137, 'possible': 1138, 'saari': 1139, 'result': 1140, 'hona': 1141, 'cha': 1142, 'mahnat': 1143, 'bde': 1144, 'hard': 1145, 'come': 1146, 'plan': 1147, 'inshallha': 1148, 'banane': 1149, 'patel': 1150, 'nonsense': 1151, 'worse': 1152, 'hate': 1153, 'starr': 1154, 'gadho': 1155, 'batao': 1156, 'president': 1157, 'news': 1158, 'channel': 1159, 'dekhta': 1160, 'because': 1161, 'samay': 1162, 'rathee': 1163, 'faida': 1164, 'bhaiya': 1165, 'message': 1166, 'bhejte': 1167, 'rahenge': 1168, 'ms': 1169, 'hoke': 1170, 'mrs': 1171, 'blessing': 1172, 've': 1173, 'done': 1174, 'would': 1175, 'die': 1176, 'cool': 1177, 'literally': 1178, 'own': 1179, 'dhamki': 1180, 'government': 1181, 'kyon': 1182, 'matlab': 1183, 'english': 1184, 'karte': 1185, 'un': 1186, 'jayada': 1187, 'whatsapp': 1188, 'rakhe': 1189, 'awaaz': 1190, 'buy': 1191, 'level': 1192, 'nahee': 1193, 'bulaya': 1194, 'tou': 1195, 'kha': 1196, 'wish': 1197, 'khrab': 1198, 'more': 1199, 'full': 1200, '20': 1201, 'crore': 1202, 'pure': 1203, 'mile': 1204, 'vo': 1205, 'moment': 1206, 'singh': 1207, 'says': 1208, 'launda': 1209, 'serious': 1210, 'totally': 1211, 'loved': 1212, 'swear': 1213, 'fav': 1214, 'nothing': 1215, 'without': 1216, 'really': 1217, 'grateful': 1218, 'isliye': 1219, 'kyuki': 1220, 'extra': 1221, 'big': 1222, 'hahaha': 1223, 'nay': 1224, 'z': 1225, 'sa': 1226, 'aukaat': 1227, 'lena': 1228, 'tujhse': 1229, 'king': 1230, 'jaisa': 1231, 'es': 1232, 'sath': 1233, 'social': 1234, 'khuda': 1235, 'mard': 1236, 'chalna': 1237, 'dun': 1238, 'shaheed': 1239, 'spread': 1240, 'humanity': 1241, 'human': 1242, 'power': 1243, 'hahahaha': 1244, 'maulana': 1245, 'mudde': 1246, 'bache': 1247, 'hee': 1248, 'topic': 1249, 'kia': 1250, 'sonu': 1251, '15': 1252, 'tickets': 1253, 'doge': 1254, 'cong': 1255, 'am': 1256, 'randi': 1257, 'humesha': 1258, 'around': 1259, 'wait': 1260, 'pyari': 1261, 'myself': 1262, 'face_with_hand_over_mouth': 1263, 'swaraj': 1264, '1': 1265, 'pucho': 1266, 'mla': 1267, 'kaunsa': 1268, 'ladkiyan': 1269, 'cigarette': 1270, 'kitni': 1271, 'lagti': 1272, 'chahye': 1273, 'hisab': 1274, 'kry': 1275, 'inshaallah': 1276, 'thought': 1277, 'va': 1278, 'spirit': 1279, 'vaise': 1280, 'hal': 1281, 'dhobi': 1282, 'kutta': 1283, 'jaisi': 1284, 'time': 1285, 'study': 1286, 'rahi': 1287, 'jese': 1288, 'padh': 1289, 'payenge': 1290, 'isme': 1291, 'lag': 1292, '/': 1293, '‚Äò': 1294, 'true': 1295, 'thumbs_up': 1296, 'need': 1297, 'major': 1298, 'rehne': 1299, 'dijiye': 1300, 'rubika': 1301, 'bechara': 1302, 'bada': 1303, 'educated': 1304, 'tumhari': 1305, 'samajh': 1306, 'bahar': 1307, 'jyada': 1308, 'jor': 1309, 'dalo': 1310, 'go': 1311, 'lol': 1312, '300': 1313, 'ane': 1314, 'agle': 1315, '7': 1316, 'gst': 1317, 'zindabad': 1318, 'murder': 1319, 'politics': 1320, 'grant': 1321, 'salah': 1322, 'mane': 1323, 'everything': 1324, 'next': 1325, 'ameen': 1326, 'hy': 1327, 'masoom': 1328, 'bachon': 1329, 'chu': 1330, 'there': 1331, 'people': 1332, 'forget': 1333, 'blockbuster': 1334, 'series': 1335, 'lagaye': 1336, '3': 1337, 'usko': 1338, 'karwa': 1339, '!!!': 1340, 'kam': 1341, 'word': 1342, 'esiliye': 1343, 'nikalta': 1344, 'ku': 1345, 'tujhe': 1346, 'mtlb': 1347, 'katrina': 1348, 'maha': 1349, 'actress': 1350, 'admit': 1351, 'surat': 1352, 'jayegi': 1353, 'naukri': 1354, 'keeps': 1355, 'bakchodi': 1356, 'sindhi': 1357, 'saans': 1358, 'lene': 1359, 'hawa': 1360, 'fans': 1361, 'kadi': 1362, 'pradesh': 1363, 'mila': 1364, 'mishra': 1365, 'maharaj': 1366, 'captain': 1367, 'statement': 1368, 'saabit': 1369, 'rhi': 1370, 'pehla': 1371, 'buri': 1372, 'shi': 1373, 'things': 1374, 'ppl': 1375, 'take': 1376, 'su': 1377, 'khelna': 1378, 'denge': 1379, 'uh': 1380, 'dhoni': 1381, 'iske': 1382, 'foreign': 1383, 'leaders': 1384, 'uncle': 1385, 'bnaya': 1386, '......': 1387, 'prem': 1388, 'dekhte': 1389, 'hue': 1390, 'saint': 1391, 'rahim': 1392, 'insan': 1393, 'dera': 1394, 'yung': 1395, 'crush': 1396, '‚Äú': 1397, 'waiting': 1398, '‚Äù': 1399, 'felt': 1400, 'ia': 1401, 'aapki': 1402, 'shat': 1403, 'naman': 1404, 'matram': 1405, 'mata': 1406, 'chawal': 1407, 'pouting_face': 1408, '1992': 1409, 'wc': 1410, 'sleepy_face': 1411, '..': 1412, 'logon': 1413, 'el': 1414, 'kehne': 1415, 'asal': 1416, 'naye': 1417, 'islam': 1418, 'paaji': 1419, 'dar': 1420, 'hamesha': 1421, 'saamne': 1422, 'dna': 1423, '>': 1424, 'playing': 1425, \"he's\": 1426, 'baby': 1427, 'boy': 1428, 'must': 1429, 'favorite': 1430, 'bajrangi': 1431, 'sultan': 1432, 'film': 1433, '5th': 1434, 'june': 1435, 'well': 1436, 'ignore': 1437, 'sushma': 1438, \"ma'am\": 1439, 'kind': 1440, 'change': 1441, 'entire': 1442, '..!!': 1443, 'long': 1444, 'during': 1445, 'elections': 1446, 'jaise': 1447, 'jeeta': 1448, 'piche': 1449, 'awaam': 1450, 'ate': 1451, 'likhte': 1452, 'jashn': 1453, 'jise': 1454, 'mara': 1455, 'mullo': 1456, 'beti': 1457, 'salute': 1458, 'brave': 1459, 'kehti': 1460, 'why': 1461, 'voh': 1462, 'itni': 1463, 'found': 1464, 'awesome': 1465, 'mohabbat': 1466, 'jaunga': 1467, 'orat': 1468, 'account': 1469, 'nadi': 1470, '5sal': 1471, 'report': 1472, 'deta': 1473, 'talking': 1474, 'jhoot': 1475, 'ever': 1476, 'made': 1477, 'away': 1478, 'cuz': 1479, 'wrong': 1480, 'jit': 1481, 'hindustan': 1482, 'future': 1483, 'mujhe': 1484, 'admi': 1485, 'looking': 1486, 'forward': 1487, 'adarsh': 1488, 'marne': 1489, 'yogi': 1490, 'cm': 1491, 'bolti': 1492, 'jodi': 1493, 'pradhan': 1494, 'panauti': 1495, 'south': 1496, 'africa': 1497, 'number': 1498, 'kiska': 1499, 'viswas': 1500, 'rohit': 1501, 'kah': 1502, 'etne': 1503, 'dikha': 1504, 'aazadi': 1505, 'deni': 1506, 'ppp': 1507, 'nabi': 1508, 'lay': 1509, 'cheez': 1510, 'karlo': 1511, 'celebrity': 1512, 'actor': 1513, 'aapako': 1514, 'purna': 1515, 'hit': 1516, 'dede': 1517, 'siddiqui': 1518, 'baap': 1519, 'nam': 1520, 'sunkar': 1521, 'chup': 1522, 'bsdk': 1523, 'makes': 1524, 'gay': 1525, 'tune': 1526, 'insult': 1527, 'girls': 1528, 'en': 1529, 'dan': 1530, 'kon': 1531, 'ramjaan': 1532, 'pura': 1533, 'ganga': 1534, 'jaate': 1535, 'aw': 1536, 'sweet': 1537, 'seeing': 1538, 'after': 1539, 'congrats': 1540, 'backhand_index_pointing_right': 1541, 'rose': 1542, 'sharm': 1543, 'pappu': 1544, 'kyoki': 1545, 'jeete': 1546, 'wajah': 1547, 'aayi': 1548, 'seema': 1549, 'kuchh': 1550, 'behan': 1551, 'thappad': 1552, 'saare': 1553, 'bro': 1554, 'delete': 1555, 'inhi': 1556, 'maine': 1557, 'banaya': 1558, 'lode': 1559, 'pedaish': 1560, 'opposition': 1561, 'kisliye': 1562, 'chuki': 1563, 'slogan': 1564, 'rehna': 1565, 'dusre': 1566, 'political': 1567, 'under': 1568, 'najar': 1569, 'could': 1570, 'offer': 1571, 'legend': 1572, 'bura': 1573, 'bhala': 1574, 'yha': 1575, 'ht': 1576, 'dekhi': 1577, 'jaaye': 1578, 'yahan': 1579, 'paisa': 1580, 'dunia': 1581, 'magar': 1582, 'wow': 1583, 'sharing': 1584, 'harkat': 1585, 'winking_face_with_tongue': 1586, 'han': 1587, 'agli': 1588, 'eidi': 1589, 'agr': 1590, 'rolling_on_the_floor_laughing': 1591, 'unfollow': 1592, 'ghatia': 1593, 'aurat': 1594, 'maharashtra': 1595, 'aake': 1596, 'dikhaye': 1597, 'hind': 1598, 'aapse': 1599, 'request': 1600, 'game': 1601, 'dena': 1602, 'chah': 1603, 'loog': 1604, 'corruption': 1605, 'nazar': 1606, 'ayi': 1607, 'yehi': 1608, 'patrakar': 1609, 'jis': 1610, 'vipaksh': 1611, 'first': 1612, 'inspiration': 1613, 'hindi': 1614, 'language': 1615, 'choice': 1616, 'whom': 1617, 'tuj': 1618, 'marega': 1619, 'waise': 1620, 'tab': 1621, 'takes': 1622, 'oath': 1623, 'group': 1624, 'local': 1625, 'add': 1626, 'taki': 1627, 'crime': 1628, 'jagah': 1629, 'bhav': 1630, 'nawaz': 1631, 'dia': 1632, 'jald': 1633, 'jaegi': 1634, 'mazak': 1635, 'said': 1636, 'baki': 1637, 'spokesperson': 1638, 'tv': 1639, 'unko': 1640, 'jawab': 1641, 'den': 1642, 'nepal': 1643, 'ag': 1644, 'bass': 1645, 'sawaal': 1646, 'lakin': 1647, 'pick': 1648, 'pic': 1649, 'bhn': 1650, 'bna': 1651, 'leti': 1652, 'sahib': 1653, 'himself': 1654, 'met': 1655, 'guru': 1656, 'form': 1657, 'kay': 1658, 'bolta': 1659, 'imaan': 1660, 'bech': 1661, 'maqsad': 1662, 'sha': 1663, 'hardik': 1664, 'lane': 1665, 'thik': 1666, 'start': 1667, 'smile': 1668, 'naa': 1669, 'boys': 1670, 'played': 1671, 'meme': 1672, 'winking_face': 1673, 'pakvwi': 1674, 'wivpak': 1675, 'jake': 1676, 'nikal': 1677, 'idhar': 1678, 'tumlog': 1679, 'drohi': 1680, 'population': 1681, 'twitter': 1682, 'bhag': 1683, '@__': 1684, 'might': 1685, 'way': 1686, 'sparkling_heart': 1687, '‚Ä¶': 1688, 'beaming_face_with_smiling_eyes': 1689, 'kuty': 1690, 'dum': 1691, 'tarha': 1692, 'age': 1693, 'skty': 1694, 'kai': 1695, 'larki': 1696, 'dedicated': 1697, 'almost': 1698, 'years': 1699, 'confirm': 1700, 'team': 1701, 'ptm': 1702, 'jaye': 1703, 'usk': 1704, 'original': 1705, 'lal': 1706, 'giving': 1707, 'jawan': 1708, 'exactly': 1709, 'season': 1710, 'until': 1711, '4-5': 1712, 'episodes': 1713, 'ago': 1714, 'achche': 1715, 'show': 1716, 'block': 1717, 'jayega': 1718, 'definitely': 1719, 'healthy': 1720, 'waheguru': 1721, 'fateh': 1722, 'via': 1723, 'tul': 1724, 'wida': 1725, 'revolving_hearts': 1726, 'friday': 1727, 'bring': 1728, 'peace': 1729, 'training': 1730, 'tabhi': 1731, 'bhart': 1732, 'jesa': 1733, 'bhiya': 1734, 'sunte': 1735, 'hein': 1736, 'smiling_face_with_smiling_eyes': 1737, 'cherry_blossom': 1738, 'mam': 1739, 'poore': 1740, 'kise': 1741, 'hats': 1742, 'defend': 1743, 'gul': 1744, 'vaha': 1745, 'vala': 1746, 'haal': 1747, 'hoa': 1748, 'zinda': 1749, 'chutiyapa': 1750, 'chala': 1751, 'rakha': 1752, 'faltu': 1753, 'bakwaas': 1754, 'samjh': 1755, 'shows': 1756, 'anchors': 1757, 'kyo': 1758, 'faila': 1759, 'chahta': 1760, 'yaha': 1761, 'dance': 1762, 'tarike': 1763, 'krte': 1764, 'terrorists': 1765, 'gayab': 1766, 'enko': 1767, 'bataya': 1768, 'bade': 1769, 'karma': 1770, 'vi': 1771, 'dhoka': 1772, 'evm': 1773, 'prachar': 1774, 'hae': 1775, 'jisne': 1776, 'jitaya': 1777, 'loktantra': 1778, 'bachao': 1779, 'tujh': 1780, 'dalal': 1781, 'sala': 1782, 'badha': 1783, 'itna': 1784, 'wanna': 1785, 'hoooo': 1786, 'chota': 1787, 'sharif': 1788, 'iski': 1789, 'nasal': 1790, 'pti': 1791, 'sindh': 1792, 'mey': 1793, 'banaye': 1794, 'behad': 1795, 'bakwas': 1796, 'respect': 1797, 'simple': 1798, 'milega': 1799, 'aab': 1800, 'karty': 1801, '2014': 1802, 'against': 1803, 'lye': 1804, 'huaa': 1805, 'dekha': 1806, 'fire': 1807, 'garam': 1808, 'apki': 1809, 'kudos': 1810, 'sahb': 1811, 'pehly': 1812, 'thy': 1813, 'jarur': 1814, 'solve': 1815, 'karenge': 1816, 'apko': 1817, 'dinner': 1818, 'char': 1819, 'aimim': 1820, 'khula': 1821, 'OK_hand': 1822, 'hundred_points': 1823, 'music': 1824, '...': 1825, 'chhote': 1826, 'ali': 1827, 'posting': 1828, 'stories': 1829, 'gov': 1830, 'sohne': 1831, 'satgur': 1832, 'sewa': 1833, 'simran': 1834, 'parmarth': 1835, 'waale': 1836, 'ghanta': 1837, 'aya': 1838, 'rally': 1839, 'chance': 1840, 'random': 1841, 'incredible': 1842, 'price': 1843, 'plus': 1844, 'jihadi': 1845, 'feku': 1846, 'gai': 1847, 'puri': 1848, 'tech': 1849, 'help': 1850, 'corrupt': 1851, 'kerte': 1852, 'lia': 1853, 'lanat': 1854, 'te': 1855, 'patwari': 1856, 'khu': 1857, 'kita': 1858, 'almighty': 1859, 'papa': 1860, 'evergreen_tree': 1861, 'plz': 1862, 'sumiran': 1863, 'many': 1864, 'never': 1865, 'hold': 1866, 'road': 1867, 'aaiye': 1868, 'stream': 1869, 'ammi': 1870, '***': 1871, 'thinker': 1872, 'daadi': 1873, 'tinka': 1874, 'win': 1875, 'jaan': 1876, 'dosti': 1877, 'man_and_woman_holding_hands': 1878, 'biwi': 1879, 'fauji': 1880, \"you're\": 1881, 'bhaijaan': 1882, 'yours': 1883, 'body': 1884, 'step': 1885, 'mukt': 1886, 'burnol': 1887, 'pay': 1888, 'patrkar': 1889, 'chamcha': 1890, 'tuje': 1891, 'bta': 1892, 'du': 1893, 'mathura': 1894, 'raw': 1895, 'khilaaf': 1896, 'andolan': 1897, 'chara': 1898, 'tamam': 1899, 'jumma': 1900, 'faiz': 1901, 'lallu': 1902, 'gaand': 1903, 'chali': 1904, 'hat': 1905, 'aik': 1906, 'zaroorat': 1907, 'hay': 1908, 'baqi': 1909, 'jungkook': 1910, 'hugs': 1911, 'special': 1912, 'him': 1913, 'dekhiye': 1914, 'train': 1915, 'hatya': 1916, 'dhayan': 1917, 'seva': 1918, 'tell': 1919, 'gee': 1920, 'face_blowing_a_kiss': 1921, 'truth': 1922, 'rainbow': 1923, 'expecting': 1924, 'women': 1925, 'cut': 1926, 'ta': 1927, 'mahila': 1928, 'karya': 1929, 'hamne': 1930, 'gujarat': 1931, 'javab': 1932, 'create': 1933, 'image': 1934, 'mind': 1935, 'hear': 1936, 'gale': 1937, 'shayad': 1938, 'janam': 1939, 'offl': 1940, 'kio': 1941, 'mujy': 1942, 'btaa': 1943, 'tang': 1944, 'lgta': 1945, 'gonna': 1946, 'pop': 1947, 'sounds': 1948, 'dj': 1949, 'bies': 1950, 'sad': 1951, 'son': 1952, 'warm': 1953, 'lovely': 1954, 'means': 1955, 'alot': 1956, 'bhole': 1957, 'hath': 1958, 'bhutto': 1959, 'chori': 1960, 'karachi': 1961, 'suwar': 1962, 'bachay': 1963, 'trending': 1964, 'yes': 1965, 'smiling_face_with_sunglasses': 1966, 'Pakistan': 1967, 'called': 1968, 'alhamdulillah': 1969, 'chand': 1970, 'khane': 1971, 'musalmaan': 1972, 'bhee': 1973, 'terrorism': 1974, 'dunya': 1975, 'badnam': 1976, 'harwa': 1977, 'state': 1978, 'saaf': 1979, 'kara': 1980, 'patrag': 1981, 'zero': 1982, 'balance': 1983, 'chuka': 1984, 'gundo': 1985, 'nahii': 1986, '‚Äî': 1987, 'super': 1988, 'busy': 1989, 'superstar': 1990, 'salmankhan': 1991, 'promotion': 1992, 'kami': 1993, 'aa': 1994, 'bhakto': 1995, 'aese': 1996, 'satta': 1997, 'sawal': 1998, 'mann': 1999, 'aai': 2000, 'kid': 2001, 'knows': 2002, 'act': 2003, 'bara': 2004, 'chootiya': 2005, 'bcz': 2006, 'terror': 2007, 'maarne': 2008, 'prateek': 2009, 'madarchod': 2010, 'jaldi': 2011, 'bi': 2012, 'socha': 2013, 'wohi': 2014, 'pareshaan': 2015, 'gud': 2016, 'milni': 2017, 'ager': 2018, '.......': 2019, 'live': 2020, 'missed': 2021, 'duaa': 2022, 'namaz': 2023, 'hn': 2024, 'la': 2025, 'ziyada': 2026, 'hei': 2027, 'rahey': 2028, 'wants': 2029, 'rohatgi': 2030, 'bilkul': 2031, 'accha': 2032, 'countries': 2033, 'shameless': 2034, 'aziz': 2035, 'milti': 2036, '18': 2037, 'gae': 2038, 'weary_face': 2039, 'yaad': 2040, 'bhul': 2041, 'over': 2042, 'banana': 2043, 'niyat': 2044, 'men': 2045, 'zindagi': 2046, 'bhot': 2047, 'achi': 2048, 'story': 2049, 'forever': 2050, 'cant': 2051, \"world's\": 2052, 'bhayya': 2053, 'ghalat': 2054, 'pichhe': 2055, 'chhod': 2056, 'sarfarazahmed': 2057, 'wah': 2058, 'dress': 2059, 'clapping_hands': 2060, 'sarfaraz': 2061, 'wehavewewill': 2062, 'cwc19': 2063, 'wishing': 2064, 'fun': 2065, 'experience': 2066, 'nah': 2067, 'right': 2068, 'usse': 2069, 'zany_face': 2070, 'haryana': 2071, 'asli': 2072, 'visvas': 2073, 'nhe': 2074, 'karan': 2075, 'instead': 2076, 'kiran': 2077, 'pita': 2078, '4th': 2079, 'exam': 2080, 'sambit': 2081, 'patra': 2082, 'jaoge': 2083, 'li': 2084, 'tumne': 2085, 'politician': 2086, 'journalists': 2087, 'bal': 2088, 'hehehe': 2089, 'won': 2090, 'lose': 2091, 'anything': 2092, 'anyways': 2093, 'cg': 2094, 'akhir': 2095, 'jeena': 2096, 'grinning_face': 2097, 'achy': 2098, 'naya': 2099, 'wajha': 2100, 'acting': 2101, 'kamaal': 2102, 'saat': 2103, 'fm': 2104, 'jate': 2105, 'isi': 2106, 'bhartiya': 2107, 'hame': 2108, 'down': 2109, 'aayega': 2110, 'minutes': 2111, 'paper': 2112, 'ankhon': 2113, 'aage': 2114, 'jaane': 2115, 'persevering_face': 2116, 'wi': 2117, 'jani': 2118, 'karti': 2119, 'krishna': 2120, 'dikh': 2121, 'city': 2122, 'seedha': 2123, 'parliament': 2124, 'bill': 2125, 'ar': 2126, 'chalta': 2127, 'sudhar': 2128, 'reservation': 2129, 'pain': 2130, 'chalega': 2131, 'umeed': 2132, 'sanjay': 2133, 'worldnotobaccoday': 2134, 'saying': 2135, 'gets': 2136, 'vohi': 2137, '4': 2138, \"that's\": 2139, 'umer': 2140, 'apun': 2141, 'mae': 2142, 'appka': 2143, 'most': 2144, 'welcome': 2145, 'ramzan': 2146, 'tm': 2147, 'got': 2148, 'skin': 2149, 'urdu': 2150, 'hare': 2151, 'haro': 2152, 'worldcup': 2153, 'jeeto': 2154, 'likh': 2155, '...‚Ä¶': 2156, 'chat': 2157, 'thak': 2158, 'becoming': 2159, 'beating_heart': 2160, \"don't\": 2161, 'interact': 2162, 'pero': 2163, 'become': 2164, 'jiska': 2165, 'uth': 2166, 'kisiko': 2167, 'position': 2168, 'century': 2169, \"isn't\": 2170, '?‚Ä¶': 2171, 'icc': 2172, 'cry': 2173, 'rs': 2174, '200': 2175, 'galti': 2176, 'raga': 2177, 'namo': 2178, 'sapne': 2179, 'partying_face': 2180, 'everyone': 2181, 'double': 2182, 'gawar': 2183, 'akki': 2184, 'cho': 2185, 'omg': 2186, 'some': 2187, 'khadi': 2188, '___': 2189, 'jitni': 2190, 'poora': 2191, 'pmln': 2192, 'zada': 2193, 'gaddar': 2194, 'chun': 2195, 'inme': 2196, 'sabit': 2197, '???': 2198, 'vapas': 2199, 'ladai': 2200, 'hinduo': 2201, 'jholi': 2202, 'aapk': 2203, 'naseeb': 2204, 'vese': 2205, 'jitna': 2206, 'kra': 2207, 'andr': 2208, 'uska': 2209, 'shukriya': 2210, 'boss': 2211, 'imandar': 2212, 'value': 2213, 'nayi': 2214, 'fursat': 2215, 'jung': 2216, 'maidan': 2217, 'blood': 2218, '6': 2219, 'zarurat': 2220, 'saccha': 2221, 'kahi': 2222, 'jhuth': 2223, 'nitish': 2224, 'kumar': 2225, 'bahumat': 2226, 'sister': 2227, 'article': 2228, 'jana': 2229, 'insaf': 2230, '2024': 2231, 'sports': 2232, 'ayega': 2233, 'jata': 2234, 'flexed_biceps': 2235, 'call': 2236, \"doesn't\": 2237, 'proof': 2238, 'bekaar': 2239, 'thay': 2240, 'acha': 2241, 'mr': 2242, 'father': 2243, 'surya': 2244, 'absolutely': 2245, 'second': 2246, 'krny': 2247, 'atleast': 2248, 'dekhti': 2249, 'istifa': 2250, 'kewal': 2251, 'ang': 2252, 'honge': 2253, 'mother': 2254, 'gave': 2255, 'birth': 2256, 'became': 2257, 'journalist': 2258, 'hin': 2259, 'anchor': 2260, 'bahot': 2261, 'abh': 2262, 'araam': 2263, 'worry': 2264, 'folded_hands_light_skin_tone': 2265, 'decision': 2266, 'farq': 2267, 'jee': 2268, 'inka': 2269, 'muqabla': 2270, 'marzi': 2271, 'paisy': 2272, 'lagao': 2273, 'bilal': 2274, 'abb': 2275, 'industry': 2276, 'karegi': 2277, 'beta': 2278, 'vichar': 2279, 'chahe': 2280, 'cr': 2281, 'hissa': 2282, 'served': 2283, 'ministers': 2284, 'prayer': 2285, 'bolo': 2286, 'mc': 2287, 'ghar': 2288, 'ghat': 2289, 'batting': 2290, 'akela': 2291, 'azam': 2292, 'aesy': 2293, 'muhammadpti': 2294, 'py': 2295, 'aaye': 2296, 'jeetne': 2297, 'bahi': 2298, 'aapka': 2299, 'bhout': 2300, 'idol': 2301, 'motivation': 2302, 'baal': 2303, 'jaya': 2304, 'kaat': 2305, 'alawa': 2306, 'tumhara': 2307, 'agree': 2308, 'kamzor': 2309, 'lov': 2310, 'prediction': 2311, 'bangladesh': 2312, 'jeetegi': 2313, 'return': 2314, 'grimacing_face': 2315, 'ngk': 2316, 'yesterday': 2317, 'months': 2318, 'dikhta': 2319, 'worker': 2320, 'workers': 2321, 'salary': 2322, 'gas': 2323, 'prabhu': 2324, 'maalik': 2325, 'kafi': 2326, 'dino': 2327, 'baate': 2328, 'bollywood': 2329, 'were': 2330, 'swearing': 2331, 'ceremony': 2332, 'shahid': 2333, 'kapoor': 2334, 'camera': 2335, 'rahne': 2336, 'once': 2337, 'kisko': 2338, 'chale': 2339, 'aate': 2340, 'haat': 2341, 'pakad': 2342, 'chaddi': 2343, 'bday': 2344, 'lit': 2345, 'af': 2346, 'agenda': 2347, 'keep': 2348, 'pani': 2349, 'jang': 2350, 'thori': 2351, 'beth': 2352, 'gey': 2353, 'muskil': 2354, 'purani': 2355, 'nehi': 2356, 'wahi': 2357, 'ministry': 2358, \"didn't\": 2359, 'active': 2360, 'daala': 2361, 'aam': 2362, \"we'll\": 2363, 'let': 2364, 'musical_notes': 2365, 'sabka': 2366, 'vikas': 2367, 'bam': 2368, 'fine': 2369, 'heart_suit': 2370, 'bb': 2371, 'thodi': 2372, 'strong': 2373, 'couple': 2374, 'days': 2375, '(@': 2376, 'open': 2377, 'event': 2378, 'dat': 2379, 'guy': 2380, 'mission': 2381, 'mangal': 2382, 'lmao': 2383, 'sooo': 2384, 'kisano': 2385, 'death': 2386, 'bata': 2387, 'deya': 2388, 'paid': 2389, 'lakho': 2390, 'lekr': 2391, 'weekend': 2392, 'stupid': 2393, 'bapu': 2394, 'appreciation': 2395, 'glad': 2396, 'relieved_face': 2397, 'pati': 2398, 'ladkiyon': 2399, 'ladke': 2400, 'shut': 2401, 'read': 2402, 'dumb': 2403, 'bitch': 2404, 'knew': 2405, 'sansad': 2406, 'ajj': 2407, 'area': 2408, '27': 2409, 'da': 2410, 'ty': 2411, 'twt': 2412, 'angel': 2413, 'purple': 2414, 'hair': 2415, 'wide': 2416, 'purple_heart': 2417, 'joy': 2418, 'sikho': 2419, 'bole': 2420, 'ekta': 2421, 'congres': 2422, 'ghamand': 2423, 'ghulam': 2424, 'pradhanmantri': 2425, 'stay': 2426, 'kitno': 2427, 'haath': 2428, 'propaganda': 2429, 'agent': 2430, 'chunav': 2431, 'lagane': 2432, 'll': 2433, 'baccha': 2434, 'bo': 2435, 'gaali': 2436, 'tag': 2437, 'sharma4': 2438, 'andhbhakt': 2439, 'asa': 2440, 'jati': 2441, 'loves': 2442, 'suru': 2443, 'sang': 2444, 'lagi': 2445, 'lil': 2446, 'bit': 2447, 'talk': 2448, 'line': 2449, 'doc': 2450, 'safe': 2451, 'flushed_face': 2452, 'fark': 2453, 'bhale': 2454, 'zardari': 2455, 'gdp': 2456, 'padta': 2457, 'unemployment': 2458, 'high': 2459, 'education': 2460, 'degrees': 2461, 'cares': 2462, 'himat': 2463, 'bande': 2464, 'dekhate': 2465, 'bowling': 2466, 'siddhu': 2467, 'oh': 2468, 'security': 2469, 'bh': 2470, 'hadh': 2471, 'rashid': 2472, 'hanuman': 2473, 'bheem': 2474, 'having': 2475, 'attitude': 2476, 'idk': 2477, 'trade_mark': 2478, 'tears': 2479, 'sone': 2480, 'board': 2481, 'fail': 2482, 'mummy': 2483, 'reaction': 2484, 'socho': 2485, 'thegiftofeducation': 2486, 'thanku': 2487, 'pate': 2488, 'supporter': 2489, 'bahir': 2490, 'qoum': 2491, 'haq': 2492, 'chhavi': 2493, 'yadav': 2494, 'pariwar': 2495, 'blessed': 2496, 'hamesa': 2497, 'rahain': 2498, 'sabki': 2499, 'umar': 2500, 'students': 2501, 'kismat': 2502, 'run': 2503, '70': 2504, 'barabar': 2505, 'humne': 2506, 'runs': 2507, 'broken_heart': 2508, 'hisaab': 2509, 'milne': 2510, 'valuetime': 2511, 'valuelife': 2512, 'motivate': 2513, 'put': 2514, 'owaisi': 2515, 'milte': 2516, 'janab': 2517, 'town': 2518, 'load': 2519, 'islamabad': 2520, 'sham': 2521, 'gujrat': 2522, 'ky': 2523, 'mehsoos': 2524, 'baaki': 2525, 'chowkidaar': 2526, 'jayenge': 2527, 'fo': 2528, 'hibiscus': 2529, 'khalid': 2530, 'mazhab': 2531, 'konsa': 2532, 'gha': 2533, 'insaan': 2534, 'yea': 2535, 'paida': 2536, 'lijiye': 2537, 'karwaya': 2538, 'jaiye': 2539, 'since': 2540, 'rajniti': 2541, 'dam': 2542, 'avi': 2543, 'jaishreeram': 2544, 'siwa': 2545, 'option': 2546, 'england': 2547, '54': 2548, 'bnai': 2549, 'smiling_face_with_horns': 2550, 'unamused_face': 2551, 'sis': 2552, 'kehta': 2553, 'purane': 2554, 'service': 2555, 'award': 2556, 'chaheye': 2557, 'trailer': 2558, 'kyunki': 2559, '√™': 2560, '√©': 2561, 'ase': 2562, 'jin': 2563, 'inki': 2564, 'appki': 2565, 'loads': 2566, 'brother': 2567, 'hokar': 2568, 'husband': 2569, 'chod': 2570, 'smjh': 2571, 'tumhare': 2572, 'bato': 2573, 'genuinely': 2574, 'child': 2575, 'words': 2576, 'bure': 2577, 'door': 2578, 'rice': 2579, 'frowning_face': 2580, 'think': 2581, '2002': 2582, 'later': 2583, 'tweeter': 2584, 'gareeb': 2585, '2020': 2586, 'kahe': 2587, 'fix': 2588, 'milegi': 2589, 'barbad': 2590, 'rhenge': 2591, 'hmesa': 2592, 'dafa': 2593, 'theek': 2594, 'vale': 2595, 'vidhansabha': 2596, 'obc': 2597, 'x': 2598, 'annaji': 2599, 'dhokha': 2600, 'dosto': 2601, 'australia': 2602, 'manga': 2603, 'chaar': 2604, 'wapis': 2605, 'aisay': 2606, 'lega': 2607, 'isiliye': 2608, 'zaleel': 2609, 'mahan': 2610, 'late': 2611, 'secularism': 2612, 'aacha': 2613, 'mili': 2614, 'pir': 2615, 'text': 2616, 'milta': 2617, 'able': 2618, 'taking': 2619, 'aakhir': 2620, 'gulam': 2621, 'sri': 2622, 'house': 2623, 'sadak': 2624, 'rahte': 2625, '105': 2626, 'poori': 2627, 'darr': 2628, 'paa': 2629, 'play': 2630, 'banao': 2631, 'nautanki': 2632, 'going': 2633, 'fayda': 2634, 'avneil': 2635, 'wearing': 2636, 'colour': 2637, 'looked': 2638, 'hatke': 2639, 'congressi': 2640, 'samj': 2641, 'nikle': 2642, 'gen': 2643, 'fauj': 2644, 'smirking_face': 2645, 'mahadev': 2646, 'unka': 2647, 'thefilm': 2648, 'karni': 2649, 'nikla': 2650, 'bayan': 2651, 'dard': 2652, 'aankh': 2653, 'bha': 2654, 'manish': 2655, 'itihaas': 2656, 'sikhaya': 2657, 'bete': 2658, 'malum': 2659, 'mene': 2660, 'chehre': 2661, 'atal': 2662, 'hmara': 2663, 'unique': 2664, 'batein': 2665, 'humara': 2666, 'cannot': 2667, 'stress': 2668, 'enough': 2669, 'care': 2670, 'hoker': 2671, 'bolna': 2672, 'jaa': 2673, 'challenge': 2674, 'crying_face': 2675, 'search': 2676, 'bhaga': 2677, 'bae': 2678, 'sali': 2679, 'buddhe': 2680, 'sambhal': 2681, 'criminal': 2682, 'plastic': 2683, 'scam': 2684, 'bhar': 2685, 'kale': 2686, 'uthata': 2687, 'inc': 2688, 'uske': 2689, 'bekar': 2690, 'farzi': 2691, 'dharti': 2692, 'end': 2693, 'layak': 2694, 'dusare': 2695, 'kafir': 2696, 'ziada': 2697, 'top': 2698, 'boondi': 2699, 'mess': 2700, 'ladies': 2701, 'small': 2702, 'little': 2703, 'ravi': 2704, 'watched': 2705, 'kudiye': 2706, 'jealous': 2707, 'versatile': 2708, 'praying': 2709, 'saeed': 2710, 'posts': 2711, 'react': 2712, 'waly': 2713, 'acchi': 2714, 'bhool': 2715, 'koe': 2716, 'rajnath': 2717, 'tarakki': 2718, 'lkn': 2719, 'wahan': 2720, 'khda': 2721, 'waala': 2722, 'copyright': 2723, 'kahan': 2724, 'suna': 2725, 'zara': 2726, 'sun': 2727, 'kamal': 2728, 'savarkar': 2729, 'court': 2730, 'pesh': 2731, 'commission': 2732, 'character': 2733, 'loving': 2734, 'style': 2735, 'angrejo': 2736, 'kapde': 2737, 'deshbhakt': 2738, 'nehru': 2739, 'yo': 2740, 'niggas': 2741, 'wh': 2742, 'masla': 2743, 'ge': 2744, 'culture': 2745, 'ay': 2746, 'rockstar': 2747, 'harr': 2748, 'hoty': 2749, 'ati': 2750, 'bann': 2751, 'pahle': 2752, 'dusro': 2753, 'cow': 2754, 'mn': 2755, 'shamil': 2756, 'jaihind': 2757, 'nagpur': 2758, 'mery': 2759, 'asked': 2760, 'gives': 2761, 'bane': 2762, 'trend': 2763, 'ani': 2764, 'prati': 2765, 'cell': 2766, 'complaint': 2767, 'jahil': 2768, 'phd': 2769, 'ok': 2770, 'daal': 2771, 'meat': 2772, 'pyare': 2773, 'passing': 2774, 'used': 2775, 'tor': 2776, 'imandari': 2777, 'islaam': 2778, 'bohut': 2779, 'aag': 2780, 'save': 2781, 'emotions': 2782, 'till': 2783, '16': 2784, 'rampal': 2785, 'families': 2786, 'living': 2787, 'moving': 2788, 'coaching': 2789, 'kutto': 2790, 'sing': 2791, 'swamy': 2792, 'khilaf': 2793, 'pahla': 2794, 'gia': 2795, 't20': 2796, 'eng': 2797, 'becomes': 2798, 'mast': 2799, 'duba': 2800, 'muah': 2801, 'happened': 2802, '‚Ä¶‚Ä¶': 2803, 'isay': 2804, 'baitha': 2805, 'cases': 2806, 'type': 2807, 'nsharif': 2808, 'course': 2809, 'personal': 2810, 'India': 2811, 'chalegi': 2812, 'thinking_face': 2813, 'mama': 2814, 'fb': 2815, 'mutra': 2816, 'hogi': 2817, 'comments': 2818, 'screenshot': 2819, 'anpad': 2820, 'karam': 2821, 'jale': 2822, 'supreme': 2823, 'university': 2824, 'north': 2825, 'east': 2826, 'school': 2827, 'pride': 2828, 'gaurav': 2829, 'sp': 2830, 'honorable': 2831, 'rojgar': 2832, 'mana': 2833, 'liberals': 2834, 'self': 2835, 'dusron': 2836, 'virodh': 2837, 'jhute': 2838, 'pori': 2839, 'sohail': 2840, 'surprise': 2841, 'fadnavis': 2842, 'early': 2843, 'alone': 2844, 'chaiye': 2845, 'missing': 2846, 'aao': 2847, 'biryani': 2848, 'dudh': 2849, 'deti': 2850, 'jisse': 2851, 'doubt': 2852, 'badla': 2853, 'gang': 2854, 'singing': 2855, 'speak-no-evil_monkey': 2856, 'jada': 2857, 'babes': 2858, 'bani': 2859, 'btao': 2860, '%': 2861, 'tumse': 2862, 'humor': 2863, 'bevkoof': 2864, 'chacha': 2865, 'player': 2866, 'worldcup2019': 2867, '14': 2868, 'kbhi': 2869, 'maaf': 2870, 'jal': 2871, 'listening': 2872, 'interactive': 2873, 'gaaye': 2874, 'unhone': 2875, 'aayenge': 2876, 'don': 2877, 'rat': 2878, 'reply': 2879, 'zrur': 2880, 'mamata': 2881, 'faith': 2882, 'kamyabi': 2883, 'ibadat': 2884, 'jitne': 2885, 'keya': 2886, 'student': 2887, 'marks': 2888, 'waja': 2889, 'bhik': 2890, 'mango': 2891, 'garib': 2892, 'chuke': 2893, 'ss': 2894, 'au': 2895, 'uss': 2896, 'yourself': 2897, 'iambharat': 2898, 'kh': 2899, 'ones': 2900, 'feet': 2901, 'baja': 2902, 'ese': 2903, 'hafte': 2904, 'gussa': 2905, 'behn': 2906, 'afridi': 2907, 'abba': 2908, 'maro': 2909, 'electricity': 2910, 'failane': 2911, 'koun': 2912, 'banda': 2913, '[': 2914, ']': 2915, 'kasi': 2916, 'bhabhi': 2917, 'kartay': 2918, 'jeetna': 2919, 'shoaib': 2920, 'akhtar': 2921, 'mental': 2922, 'sharab': 2923, 'padha': 2924, 'bbc': 2925, 'achieve': 2926, 'whole': 2927, 'btw': 2928, 'lad': 2929, 'smiling_face': 2930, 'piti': 2931, 'pel': 2932, 'director': 2933, \"hon'ble\": 2934, 'officer': 2935, 'using': 2936, 'jite': 2937, 'lives': 2938, 'khandan': 2939, 'insan1': 2940, 'kasam': 2941, 'chehra': 2942, 'ankho': 2943, 'aati': 2944, 'kiye': 2945, 'jute': 2946, 'dalle': 2947, 'likhna': 2948, 'brain': 2949, 'neutral_face': 2950, 'pyara': 2951, 'jaiga': 2952, 'chamche': 2953, 'chamcho': 2954, 'chilla': 2955, 'bn': 2956, 'yug': 2957, 'kalyug': 2958, 'dev': 2959, 'amethi': 2960, 'yh': 2961, 'padhai': 2962, 'gupta': 2963, 'middle_finger': 2964, 'sar': 2965, 'amir': 2966, 'speed': 2967, 'batsman': 2968, 'beat': 2969, 'jub': 2970, 'opinion': 2971, 'celebration': 2972, 'pal': 2973, 'celebrating': 2974, 'deep': 2975, 'week': 2976, 'gye': 2977, 'gaandu': 2978, 'masha': 2979, 'example': 2980, 'shikayat': 2981, 'apky': 2982, 'bhaag': 2983, 'bcha': 2984, 'ander': 2985, 'yaro': 2986, 'shandar': 2987, 'fund': 2988, 'dalit': 2989, 'beef': 2990, 'lekar': 2991, 'dunga': 2992, 'kissing_face_with_closed_eyes': 2993, 'dukhi': 2994, 'aatma': 2995, 'kissing_face_with_smiling_eyes': 2996, 'jb': 2997, 'upper': 2998, 'community': 2999, 'godi': 3000, 'kitne': 3001, 'bika': 3002, 'banne': 3003, 'clothes': 3004, 'judge': 3005, 'sirjee': 3006, 'abt': 3007, 'bht': 3008, 'br': 3009, 'baithne': 3010, 'quetta': 3011, 'sabko': 3012, 'achhe': 3013, 'mala': 3014, 'goa': 3015, 'due': 3016, 'release': 3017, 'sadi': 3018, 'samose': 3019, 'hen': 3020, 'bear': 3021, '....': 3022, 'raising_hands': 3023, 'lambi': 3024, 'kush': 3025, 'rho': 3026, '@‚Ä¶': 3027, 'code': 3028, 'etc': 3029, 'lun': 3030, 'bandh': 3031, 'wapas': 3032, 'keliye': 3033, 'abhinandan': 3034, 'sona': 3035, 'pulwama': 3036, 'murkh': 3037, \"india's\": 3038, 'tuk': 3039, 'lain': 3040, 'rn': 3041, 'tom': 3042, 'jaisy': 3043, 'naw': 3044, 'promote': 3045, 'chalti': 3046, '100': 3047, 'jhand': 3048, 'rastra': 3049, 'demand': 3050, 'ami': 3051, 'bajrangbali': 3052, 'quote': 3053, 'greatest': 3054, 'upon': 3055, 'imad': 3056, 'bd': 3057, 'roz': 3058, 'wins': 3059, 'daughter': 3060, 'bne': 3061, 'khi': 3062, 'logic': 3063, 'dilli': 3064, 'chahte': 3065, 'kejriwal': 3066, 'dusri': 3067, 'bajaye': 3068, 'tb': 3069, 'ghatya': 3070, 'trah': 3071, 'humein': 3072, 'karay': 3073, 'kahna': 3074, 'cabinet': 3075, 'sighhh': 3076, 'apnay': 3077, 'boot': 3078, 'reh': 3079, 'chapter': 3080, 'car': 3081, 'drama': 3082, 'sidha': 3083, 'jara': 3084, 'dhyan': 3085, 'des': 3086, 'gum': 3087, 'sal': 3088, 'lines': 3089, 'dad': 3090, 'chutiyo': 3091, 'business': 3092, 'waqt': 3093, 'dekhenge': 3094, 'aajadi': 3095, 'karn': 3096, 'gao': 3097, 'jabki': 3098, 'ukhad': 3099, 'sundar': 3100, 'uditraj': 3101, 'maa-baap': 3102, 'rasta': 3103, 'bachy': 3104, 'afghani': 3105, 'handle': 3106, 'rehta': 3107, 'taaza': 3108, 'nation': 3109, 'smj': 3110, 'muslman': 3111, \"we're\": 3112, 'getting': 3113, 'kindly': 3114, 'bilalabbasonjeetopakistan': 3115, 'sarkaar': 3116, 'shadi': 3117, 'modiswearingin': 3118, 'swagat': 3119, 'loksabha': 3120, 'nae': 3121, 'shout': 3122, 'listen': 3123, 'others': 3124, 'bilawal': 3125, 'members': 3126, 'lassi': 3127, 'another': 3128, 'doodh': 3129, 'pledgeagainstcorruption': 3130, 'pujya': 3131, 'emotional': 3132, 'given': 3133, '$': 3134, 'asking': 3135, 'only': 3136, 'slogans': 3137, 'color': 3138, 'tarh': 3139, 'sleeping_face': 3140, 'mumbai': 3141, 'candidate': 3142, 'sah': 3143, '2nd': 3144, '3rd': 3145, 'karwai': 3146, 'expressionless_face': 3147, 'bante': 3148, 'tarf': 3149, 'bhosda': 3150, 'hot': 3151, 'kardiya': 3152, 'skte': 3153, 'mention': 3154, 'fayada': 3155, 'haven': 3156, 'laddoo': 3157, 'jharkhand': 3158, 'balki': 3159, 'shaan': 3160, 'ps': 3161, 'suicide': 3162, 'naak': 3163, 'gaon': 3164, 'gift': 3165, 'arrest': 3166, 'jobs': 3167, 'juth': 3168, 'parha': 3169, 'samjha': 3170, 'short': 3171, 'practice': 3172, 'sirji': 3173, 'lie': 3174, 'tottenham': 3175, 'dekhunga': 3176, 'para': 3177, '√º': 3178, 'baato': 3179, 'bkwas': 3180, 'face_with_medical_mask': 3181, 'bachpan': 3182, 'bhasha': 3183, 'utna': 3184, 'banegi': 3185, 'inn': 3186, 'waalon': 3187, 'aadarniy': 3188, 'dubai': 3189, 'deserves': 3190, 'rahulgandhi': 3191, 'jayoge': 3192, 'regards': 3193, 'haa': 3194, 'tar': 3195, 'excellent': 3196, 'narendramodi': 3197, 'meet': 3198, 'letter': 3199, 'copy': 3200, 'tusi': 3201, 'idiot': 3202, 'dp': 3203, 'suar': 3204, 'saala': 3205, 'banate': 3206, 'aaplog': 3207, 'sara': 3208, 'halat': 3209, 'track': 3210, 'mout': 3211, 'ch': 3212, 'ched': 3213, 'duper': 3214, 'sacha': 3215, 'sauda': 3216, 'criminals': 3217, 'relationship': 3218, 'jaruri': 3219, 'ghus': 3220, 'azadi': 3221, 'sikh': 3222, 'sidhu': 3223, 'happens': 3224, 'amma': 3225, 'crying': 3226, 'jaake': 3227, 'aayegi': 3228, 'driver': 3229, 'station': 3230, 'maximum': 3231, 'success': 3232, 'choro': 3233, 'likhi': 3234, 'heard': 3235, 'bewise': 3236, 'chooseright': 3237, 'afternoon': 3238, 'badiya': 3239, 'roj': 3240, 'note': 3241, 'nakli': 3242, 'khatm': 3243, 'worth': 3244, 'bahu': 3245, 'size': 3246, 'inside': 3247, 'khul': 3248, 'mohsin': 3249, 'khata': 3250, 'song': 3251, 'badlega': 3252, '58': 3253, 'bhadwa': 3254, 'hurts': 3255, 'dabangg': 3256, 'tiger': 3257, 'dobara': 3258, 'shine': 3259, 'bnane': 3260, 'tat': 3261, 'kiss': 3262, 'caste': 3263, 'religion': 3264, 'chalate': 3265, 'ashutosh': 3266, 'sumrin': 3267, 'bakso': 3268, '130': 3269, 'jaga': 3270, 'zain': 3271, 'bcoz': 3272, 'sh': 3273, 'happiest': 3274, 'tatti': 3275, 'sin': 3276, 'imran': 3277, 'hashmi': 3278, 'woman': 3279, 'trump': 3280, 'feelings': 3281, 'hurt': 3282, 'hongi': 3283, 'face_with_steam_from_nose': 3284, 'bhadwe': 3285, 'behtar': 3286, 'zimbabwe': 3287, 'uae': 3288, 'indi': 3289, 'pichwada': 3290, 'shyd': 3291, 'told': 3292, 'meant': 3293, 'ju': 3294, 'hafiz': 3295, 'channels': 3296, 'inhe': 3297, 'voter': 3298, 'tameez': 3299, 'kaisy': 3300, 'hiii': 3301, 'priya': 3302, 'jali': 3303, 'behen': 3304, 'nikalo': 3305, 'abuse': 3306, 'bano': 3307, 'brigade': 3308, 'hands': 3309, 'dalali': 3310, 'foot': 3311, 'abki': 3312, 'beauty': 3313, 'queen': 3314, 'champion': 3315, '28': 3316, 'hv': 3317, 'idea': 3318, 'ladka': 3319, 'base': 3320, 'pesa': 3321, 'hoi': 3322, 'mood': 3323, 'xd': 3324, 'banayenge': 3325, 'vs': 3326, 'final': 3327, 'jitega': 3328, 'askstar': 3329, '**': 3330, 'date': 3331, 'lagte': 3332, 'ilawa': 3333, 'fikar': 3334, 'gand': 3335, 'hehehehe': 3336, 'project': 3337, 'probably': 3338, 'present': 3339, 'monday': 3340, '8': 3341, 'aka': 3342, 'hours': 3343, 'member': 3344, 'mota': 3345, 'hathi': 3346, 'laugh': 3347, 'arre': 3348, 'saheb': 3349, 'chief': 3350, 'okay': 3351, '=': 3352, 'soniya': 3353, ';': 3354, 'divine': 3355, 'mahanta': 3356, 'dwara': 3357, '07': 3358, 'dharam': 3359, 'chalane': 3360, 'rok': 3361, '23': 3362, 'press': 3363, 'kapil': 3364, 'america': 3365, 'iran': 3366, 'humse': 3367, 'mashallah': 3368, 'lord': 3369, 'remain': 3370, 'focus': 3371, \"what's\": 3372, 'amen': 3373, '!‚Ä¶': 3374, 'nobody': 3375, 'valo': 3376, 'boycott': 3377, 'iccworldcup2019': 3378, 'assalam': 3379, 'wonderful': 3380, 'lady': 3381, 'jaror': 3382, 'org': 3383, 'samajhne': 3384, 'lge': 3385, 'jrurt': 3386, 'anushka': 3387, 'zyada': 3388, 'kerala': 3389, 'left': 3390, 'bharosha': 3391, 'muft': 3392, 'terrorist': 3393, 'bharwa': 3394, 'huva': 3395, 'plzz': 3396, 'beech': 3397, 'rehti': 3398, 'exams': 3399, 'shukar': 3400, 'phela': 3401, 'eat': 3402, '|': 3403, 'mashaallah': 3404, 'hmesha': 3405, 'karana': 3406, 'abhivyakti': 3407, 'gala': 3408, 'common': 3409, 'nikalna': 3410, 'namaste': 3411, 'mari': 3412, 'khayega': 3413, 'koyi': 3414, 'dekhne': 3415, 'room': 3416, '2.': 3417, '1st': 3418, 'bhaiyo': 3419, 'kidhar': 3420, 'sanskar': 3421, 'kho': 3422, 'hamain': 3423, 'prove': 3424, 'mauke': 3425, 'bhaijan': 3426, 'slightly_smiling_face': 3427, 'padega': 3428, 'barbaad': 3429, 'banega': 3430, 'assembly': 3431, 'different': 3432, 'vry': 3433, 'mandi': 3434, 'private': 3435, 'hack': 3436, 'samjhe': 3437, 'mudda': 3438, 'sat': 3439, 'bach': 3440, 'break': 3441, 'booked': 3442, 'explain': 3443, 'mo': 3444, 'chl': 3445, 'daku': 3446, 'shareef': 3447, 'deke': 3448, 'jinhe': 3449, '||': 3450, 'status': 3451, 'youtube': 3452, 'fouj': 3453, 'baray': 3454, 'sochna': 3455, 'ghareeb': 3456, 'itny': 3457, 'haina': 3458, 'haramkhor': 3459, 'sabke': 3460, 'katne': 3461, 'poetry': 3462, 'mayawati': 3463, 'lete': 3464, 'huye': 3465, 'muh': 3466, 'kanoon': 3467, 'devi': 3468, 'apman': 3469, 'muhabbat': 3470, 'dy': 3471, 'narender': 3472, 'ndtv': 3473, \"there's\": 3474, 'appreciate': 3475, 'lok': 3476, 'rani': 3477, 'decide': 3478, 'rab': 3479, 'party_popper': 3480, 'wrapped_gift': 3481, 'ahead': 3482, 'ik': 3483, 'hussain': 3484, 'sake': 3485, 'female': 3486, 'target': 3487, 'ro': 3488, 'rahoge': 3489, 'close': 3490, 'lalu': 3491, 'aana': 3492, 'baccho': 3493, 'chahty': 3494, 'sunna': 3495, 'je': 3496, 'bss': 3497, 'ksi': 3498, 'importance': 3499, 'munh': 3500, 'jaty': 3501, 'mansoor': 3502, 'bhan': 3503, 'retweet': 3504, 'raaj': 3505, 'krenge': 3506, '1947': 3507, 'bijali': 3508, 'toilet': 3509, 'bhokne': 3510, 'jinka': 3511, 'teamindia': 3512, 'kohli': 3513, 'pine': 3514, 'face_with_rolling_eyes': 3515, 'durga': 3516, 'shakti': 3517, 'jindabad': 3518, 'normal': 3519, 'sure': 3520, 'guess': 3521, 'beloved': 3522, 'darshata': 3523, '007': 3524, 'batane': 3525, 'sehri': 3526, 'remember': 3527, '.........': 3528, 'al': 3529, 'punjabi': 3530, 'khich': 3531, 'certificate': 3532, 'jarurt': 3533, 'kanhaiya': 3534, 'goli': 3535, 'marunga': 3536, 'kiske': 3537, 'control': 3538, 'issi': 3539, 'jobless': 3540, 'bhukha': 3541, 'humare': 3542, 'went': 3543, 'hushed_face': 3544, 'safed': 3545, 'saree': 3546, 'rakshak': 3547, 'beh': 3548, 'sharkar': 3549, 'sbi': 3550, 'mom': 3551, 'thinks': 3552, 'speech': 3553, 'dikhane': 3554, 'mante': 3555, 'laat': 3556, 'maaro': 3557, 'football': 3558, 'chinta': 3559, 'data': 3560, 'remove': 3561, 'samaj': 3562, 'jaao': 3563, 'waalo': 3564, 'saalon': 3565, 'history': 3566, 'career': 3567, 'ft': 3568, 'sachche': 3569, 'kijiye': 3570, 'badhiya': 3571, 'padi': 3572, 'pairo': 3573, 'kalank': 3574, 'batana': 3575, 'abad': 3576, 'kahin': 3577, 'huyi': 3578, 'secular': 3579, 'kashyap': 3580, 'hmare': 3581, 'marte': 3582, 'vajah': 3583, '21': 3584, 'pi': 3585, 'rest': 3586, 'shubhkamnaye': 3587, 'avm': 3588, 'book': 3589, 'choti': 3590, 'intelligent': 3591, 'worked': 3592, \"everyone's\": 3593, 'development': 3594, 'max': 3595, 'akal': 3596, 'gunda': 3597, 'kash': 3598, 'minority': 3599, 'waha': 3600, 'khushi': 3601, 'law': 3602, 'policy': 3603, '303': 3604, 'mehnat': 3605, 'internet': 3606, 'alla': 3607, 'action': 3608, 'chanting': 3609, 'teach': 3610, 'maut': 3611, 'ƒ±': 3612, 'dikkat': 3613, '√ß': 3614, '√∂': 3615, 'crown': 3616, '//': 3617, 'co': 3618, 'gun': 3619, 'violence': 3620, 'madhar': 3621, 'atalji': 3622, 'badal': 3623, 'comedy': 3624, 'hogaye': 3625, 'taste': 3626, 'mumkin': 3627, 'buddhi': 3628, 'squinting_face_with_tongue': 3629, '10': 3630, 'po': 3631, 'apno': 3632, 'chle': 3633, 'jaana': 3634, 'lga': 3635, 'suvar': 3636, 'chappal': 3637, 'version': 3638, 'mandal': 3639, 'uk': 3640, 'shirt': 3641, 'fa': 3642, 'skta': 3643, 'muje': 3644, 'daikh': 3645, 'dost': 3646, 'select': 3647, 'akka': 3648, 'sunday': 3649, 'condition': 3650, 'badkar': 3651, 'rhne': 3652, 'dub': 3653, '1.': 3654, 'huge': 3655, '3.': 3656, 'parh': 3657, 'eagerly': 3658, 'naal': 3659, 'rang': 3660, 'lai': 3661, 'bari': 3662, 'iftar': 3663, 'face_savoring_food': 3664, 'feeling': 3665, 'hiiiii': 3666, 'tw': 3667, 'wil': 3668, 'disappointed_face': 3669, 'niche': 3670, 'utha': 3671, 'veer': 3672, 'kala': 3673, 'brings': 3674, 'screen': 3675, 'pooch': 3676, 'rhna': 3677, 'brahman': 3678, 'charan': 3679, 'zaroor': 3680, 'kissi': 3681, 'beghairat': 3682, 'tax': 3683, 'yuva': 3684, 'berojgar': 3685, 'height': 3686, 'sign': 3687, 'bachhon': 3688, 'fati': 3689, 'space': 3690, 'adha': 3691, 'manniye': 3692, 'dubara': 3693, 'ass': 3694, 'parsad': 3695, 'soti': 3696, 'janm': 3697, 'jaiso': 3698, 'mahoday': 3699, 'kisaan': 3700, 'shot': 3701, 'aunty': 3702, 'aku': 3703, '35a': 3704, 'dala': 3705, 'returns': 3706, 'jiyo': 3707, 'half': 3708, 'namah': 3709, 'teeno': 3710, 'kuttey': 3711, 'banta': 3712, 'daily': 3713, 'precious': 3714, 'village': 3715, 'backhand_index_pointing_left': 3716, 'memes': 3717, 'surjewala': 3718, 'usey': 3719, 'mauka': 3720, 'sidhi': 3721, 'ladki': 3722, 'diwali': 3723, 'exit': 3724, 'poll': 3725, 'hasi': 3726, 'tara': 3727, 'seriously': 3728, 'shameful': 3729, 'kaash': 3730, '10k': 3731, 'icon': 3732, 'turant': 3733, 'kholne': 3734, 'iam': 3735, 'china': 3736, 'nda': 3737, 'sapath': 3738, 'grahan': 3739, 'mirchi': 3740, 'bund': 3741, 'bhen': 3742, 'kese': 3743, 'musalmano': 3744, 'jannat': 3745, 'pait': 3746, 'fair': 3747, 'haraya': 3748, 'satya': 3749, 'nik': 3750, 'anurag': 3751, 'padhna': 3752, 'clear': 3753, 'swt': 3754, 'banday': 3755, 'roop': 3756, 'written': 3757, 'hug': 3758, 'krn': 3759, 'bharatwithfamily': 3760, 'calling': 3761, 'ugly': 3762, 'jameen': 3763, 'difference': 3764, 'perform': 3765, 'akshay': 3766, 'telling': 3767, 'information': 3768, 'teko': 3769, 'ada': 3770, 'art': 3771, ';)': 3772, 'though': 3773, 'dangerous': 3774, 'chiye': 3775, 'kaisa': 3776, '........': 3777, 'modisarkar2': 3778, 'loose': 3779, 'views': 3780, 'summer': 3781, 'services': 3782, 'link': 3783, 'bio': 3784, 'chakar': 3785, '30': 3786, 'rupees': 3787, 'saja': 3788, 'chatukar': 3789, 'master': 3790, 'kre': 3791, 'hindutva': 3792, 'bhagwa': 3793, 'jaahil': 3794, 'planning': 3795, 'hm': 3796, 'grinning_face_with_big_eyes': 3797, 'hmne': 3798, 'kee': 3799, 'yadi': 3800, '35': 3801, 'saalo': 3802, 'yet': 3803, 'que': 3804, '√≠': 3805, 'rajasthan': 3806, 'ghante': 3807, 'jindagi': 3808, 'badhte': 3809, 'peeche': 3810, 'chalne': 3811, 'shan': 3812, 'jabtak': 3813, 'khana': 3814, 'metro': 3815, 'nikalne': 3816, 'smaj': 3817, 'mu': 3818, 'standard': 3819, 'tumare': 3820, 'puch': 3821, 'jaroor': 3822, 'acche': 3823, 'achcha': 3824, 'khoob': 3825, 'inhone': 3826, 'reality': 3827, 'mna': 3828, 'kisne': 3829, 'bhosdi': 3830, 'aamir': 3831, 'serve': 3832, 'maray': 3833, 'teacher': 3834, 'gh': 3835, 'dey': 3836, 'bachey': 3837, 'believe': 3838, 'meeting': 3839, 'khus': 3840, 'dusari': 3841, 'apane': 3842, 'uhh': 3843, 'raju': 3844, 'heartiest': 3845, 'latest': 3846, 'evening': 3847, 'dh': 3848, 'dikhti': 3849, 'saara': 3850, 'troll': 3851, 'karoge': 3852, 'gujarati': 3853, 'marathi': 3854, 'smjhne': 3855, 'ghatna': 3856, 'papu': 3857, 'manta': 3858, 'deserve': 3859, 'raste': 3860, 'dekhega': 3861, 'coach': 3862, 'response': 3863, 'sleep': 3864, 'unhi': 3865, 'try': 3866, 'campaign': 3867, 'nesamani': 3868, 'marna': 3869, 'thore': 3870, 'worldvictorioussaint': 3871, 'vyakti': 3872, 'typical': 3873, 'stage': 3874, 'kaa': 3875, 'chakkar': 3876, 'supporting': 3877, 'yaa': 3878, 'salam': 3879, 'maafi': 3880, 'thankyou': 3881, 'mag': 3882, 'ra': 3883, 'lang': 3884, 'war': 3885, 'games': 3886, 'qki': 3887, 'bigad': 3888, 'aaap': 3889, 'km': 3890, 'kharcha': 3891, 'came': 3892, 'ravish': 3893, 'rakhne': 3894, 'chillate': 3895, 'click': 3896, 'backhand_index_pointing_down': 3897, 'ameer': 3898, 'iman': 3899, 'ijjat': 3900, 'teachers': 3901, 'saturday': 3902, 'red': 3903, 'along': 3904, 'wear': 3905, 'makeup': 3906, 'gold': 3907, 'machine': 3908, 'pichle': 3909, '45': 3910, 'berojgari': 3911, 'kb': 3912, 'srf': 3913, 'lucky': 3914, 'bewakuf': 3915, 'madharchod': 3916, 'andar': 3917, 'chla': 3918, 'gham': 3919, 'bary': 3920, 'subah': 3921, 'kehte': 3922, 'chote': 3923, 'mod': 3924, 'muhammad': 3925, 'force': 3926, 'coffee': 3927, 'stand': 3928, 'least': 3929, 'matches': 3930, '17': 3931, 'mine': 3932, '22': 3933, 'confidence': 3934, 'ai': 3935, 'fakhar': 3936, 'tour': 3937, 'choot': 3938, 'bts': 3939, 'nailed': 3940, 'created': 3941, 'bool': 3942, 'kaafir': 3943, 'leadership': 3944, 'pade': 3945, 'milke': 3946, 'sriram': 3947, 'jari': 3948, 'rakhna': 3949, 'halal': 3950, 'nikala': 3951, 'kardo': 3952, 'ohhhh': 3953, 'based': 3954, 'apney': 3955, 'kanun': 3956, 'bhrast': 3957, 'khabees': 3958, 'ae': 3959, 'seats': 3960, 'ul': 3961, 'baaj': 3962, 'joker': 3963, 'into': 3964, 'gajab': 3965, 'passport': 3966, 'dimag': 3967, 'past': 3968, 'irani': 3969, 'ee': 3970, 'fresh': 3971, 'earth': 3972, 'yojana': 3973, 'players': 3974, 'glti': 3975, 'kahne': 3976, 'cities': 3977, 'kripa': 3978, 'specially': 3979, 'doston': 3980, 'kahega': 3981, 'hata': 3982, 'mitti': 3983, 'epic': 3984, 'bda': 3985, 'maat': 3986, 'dog': 3987, 'ahmed': 3988, 'aulaad': 3989, 'new_moon_face': 3990, 'etni': 3991, 'sex': 3992, 'mujh': 3993, 'yakeen': 3994, 'krega': 3995, 'ran': 3996, 'sak': 3997, \"i'd\": 3998, 'album': 3999, 'gaane': 4000, 'model': 4001, 'mehta': 4002, 'boring': 4003, 'pallavi92': 4004, 'degi': 4005, 'jummah': 4006, 'meray': 4007, 'location': 4008, 'purana': 4009, 'rape': 4010, 'following': 4011, 'quran': 4012, 'ns': 4013, 'manhoos': 4014, 'kabi': 4015, 'dabang': 4016, 'humko': 4017, 'lagega': 4018, 'ish': 4019, 'rather': 4020, 'gar': 4021, 'haaro': 4022, 'suno': 4023, 'vishal': 4024, 'nare': 4025, 'btaya': 4026, 'ly': 4027, 'appko': 4028, 'dikhega': 4029, 'karein': 4030, 'songs': 4031, 'jiye': 4032, 'starting': 4033, 'dreams': 4034, 'doobara': 4035, 'maang': 4036, 'punjab': 4037, 'jesy': 4038, 'deny': 4039, 'fam': 4040, 'sherbet': 4041, 'helps': 4042, '500': 4043, 'candidates': 4044, 'program': 4045, 'garrywalia': 4046, 'nikaal': 4047, 'khaane': 4048, 'mian': 4049, 'zarur': 4050, 'sahe': 4051, 'congratulation': 4052, 'humari': 4053, 'cutie': 4054, 'yrrrr': 4055, 'parmatma': 4056, 'white': 4057, 'deen': 4058, 'ƒÅ': 4059, 'announcement': 4060, '2019.': 4061, 'tarif': 4062, 'garibo': 4063, 'bajane': 4064, 'collection': 4065, 'rabb': 4066, 'pro': 4067, 'acc': 4068, 'arvind': 4069, 'swara': 4070, 'sachin': 4071, 'reports': 4072, 'miley': 4073, 'hariom': 4074, 'baithe': 4075, 'dikhaya': 4076, 'bright': 4077, 'brothers': 4078, 'sisters': 4079, 'khele': 4080, 'wc19': 4081, 'sbka': 4082, 'card': 4083, 'ekdum': 4084, 'growth': 4085, 'janata': 4086, 'eye': 4087, 'dead': 4088, 'ashirwad': 4089, 'mary': 4090, 'dekar': 4091, 'touching': 4092, 'akhilesh': 4093, 'haalat': 4094, 'gayee': 4095, 'list': 4096, 'tyar': 4097, 'chupane': 4098, 'changed': 4099, 'dvn': 4100, 'complain': 4101, 'darshan': 4102, 'aapas': 4103, 'haari': 4104, 'maja': 4105, 'yakin': 4106, 'face_without_mouth': 4107, 'rona': 4108, 'voting': 4109, 'garv': 4110, 'dude': 4111, 'jaenge': 4112, 'bkl': 4113, 'chaap': 4114, 'nanga': 4115, 'lakh': 4116, 'suit': 4117, 'rule': 4118, 'yaadein': 4119, 'blue_heart': 4120, 'dharm': 4121, 'tod': 4122, 'chupke': 4123, 'current': 4124, 'khelo': 4125, '...!!!': 4126, 'dhang': 4127, 'kerne': 4128, 'jiii': 4129, 'qadar': 4130, 'oooh': 4131, 'bin': 4132, 'pathan': 4133, 'freedom': 4134, 'kahte': 4135, 'bahan': 4136, 'kholi': 4137, 'shalwar': 4138, 'rkhna': 4139, 'rahti': 4140, 'piyar': 4141, 'mga': 4142, 'expectations': 4143, 'pleasure': 4144, 'eh': 4145, 'while': 4146, 'varna': 4147, 'trophy': 4148, 'pora': 4149, 'bharosa': 4150, 'bhaut': 4151, 'kyaaa': 4152, 'decided': 4153, 'eating': 4154, 'awaz': 4155, 'rehi': 4156, 'skti': 4157, 'tareef': 4158, 'khol': 4159, 'jitegi': 4160, 'pehli': 4161, 'gaw': 4162, 'low': 4163, 'green': 4164, 'auqat': 4165, 'jariye': 4166, 'college': 4167, 'khayal': 4168, 'samna': 4169, 'supporters': 4170, 'oy': 4171, 'miya': 4172, 'vahi': 4173, 'sinha': 4174, 'tahe': 4175, 'pravakta': 4176, 'killer': 4177, 'sunoo': 4178, 'correct': 4179, 'pehlay': 4180, 'total': 4181, 'either': 4182, 'indians': 4183, 'powerful': 4184, 'ghost': 4185, 'understand': 4186, 'box': 4187, 'office': 4188, 'sharma1': 4189, 'cutest': 4190, 'content': 4191, 'afwah': 4192, '10th': 4193, 'harane': 4194, 'ishq': 4195, 'raja': 4196, 'bandar': 4197, 'ajeeb': 4198, 'yellow_heart': 4199, 'black_heart': 4200, 'kohinoor': 4201, 'jammu': 4202, 'hazar': 4203, 'pnr': 4204, 'serial': 4205, 'sunochanda2': 4206, 'bsp': 4207, 'md': 4208, 'protect': 4209, 'hindus': 4210, 'baare': 4211, 'ips': 4212, 'speaker': 4213, 'guide': 4214, 'kahenge': 4215, 'abu': 4216, 'choor': 4217, 'cheej': 4218, 'rahna': 4219, 'mobile': 4220, 'sayad': 4221, 'mujhse': 4222, 'fat': 4223, 'sardana': 4224, 'aajtak': 4225, 'ghadar': 4226, 'trp': 4227, 'notice': 4228, 'pc': 4229, 'videos': 4230, 'tery': 4231, 'chodo': 4232, 'fc': 4233, 'rozay': 4234, 'celeb': 4235, 'janwar': 4236, 'karao': 4237, 'shanti': 4238, 'htt': 4239, 'leye': 4240, 'baatein': 4241, 'krni': 4242, 'aqal': 4243, 'rahan': 4244, 'khada': 4245, 'esa': 4246, 'results': 4247, 'orange_heart': 4248, 'chutia': 4249, 'heart_with_ribbon': 4250, 'kuttay': 4251, 'sarfraz': 4252, 'yad': 4253, 'ilaj': 4254, 'jiti': 4255, 'jeeti': 4256, 'chahata': 4257, 'thats': 4258, 'takleef': 4259, 'khaana': 4260, 'milkar': 4261, 'comment': 4262, 'virodhi': 4263, 'purn': 4264, 'kabir': 4265, 'rosette': 4266, 'bakhshna': 4267, 'geo': 4268, 'pakistanio': 4269, 'page': 4270, 'madhyam': 4271, 'khabar': 4272, 'growing_heart': 4273, 'waving_hand': 4274, 'raul': 4275, 'yrs': 4276, 'british': 4277, 'karege': 4278, 'sabha': 4279, 'ahmedabad': 4280, 'faad': 4281, 'kabirsingh': 4282, 'loss': 4283, 'gal': 4284, 'udhar': 4285, 'maloom': 4286, 'sey': 4287, 'pucha': 4288, 'bagal': 4289, 'ti': 4290, 'kichad': 4291, 'grade': 4292, 'luv': 4293, 'pakka': 4294, 'chahal': 4295, 'pilot': 4296, 'politicians': 4297, 'drugs': 4298, 'worst': 4299, '2018': 4300, 'uper': 4301, 'pai': 4302, 'lover': 4303, 'jnu': 4304, 'hatho': 4305, 'gaay': 4306, 'scindia': 4307, 'interview': 4308, 'effort': 4309, 'rajneeti': 4310, 'sob': 4311, 'voted': 4312, 'kitab': 4313, 'lunch': 4314, 'bnd': 4315, 'kareena': 4316, 'speak': 4317, 'speaks': 4318, 'gaa': 4319, 'laude': 4320, 'abaad': 4321, 'filled': 4322, 'sochta': 4323, \"ma'm\": 4324, 'confetti_ball': 4325, 'fees': 4326, 'jwab': 4327, 'dety': 4328, 'bomb': 4329, 'ex': 4330, 'mercy': 4331, 'period': 4332, 'mah': 4333, 'bethe': 4334, 'saas': 4335, 'bhosdike': 4336, 'jldi': 4337, 'azamgarh': 4338, 'kas': 4339, 'hajaro': 4340, 'ahmad': 4341, 'filmy': 4342, '2288': 4343, 'haare': 4344, 'rehte': 4345, 'aditi': 4346, 'hamaare': 4347, 'bhavishya': 4348, 'realme': 4349, '-‚Ä¶': 4350, 'marenge': 4351, 'sparkles': 4352, 'noor': 4353, 'married': 4354, 'sad_but_relieved_face': 4355, 'profile': 4356, 'parents': 4357, 'subha': 4358, 'write': 4359, 'anthem': 4360, 'thoughts': 4361, 'deeds': 4362, 'ghotale': 4363, 'khelte': 4364, 'halaat': 4365, 'jinke': 4366, 'asad': 4367, 'triangular_flag': 4368, 'tulip': 4369, 'uthaya': 4370, 'mahine': 4371, '0': 4372, 'amna': 4373, 'ayenge': 4374, 'tried': 4375, 'ca': 4376, '.@': 4377, 'anniversary': 4378, 'hahahha': 4379, 'bandi': 4380, 'humble': 4381, 'white_heavy_check_mark': 4382, 'mangoes': 4383, '!?': 4384, 'jcbkikhudayi': 4385, '!!!!!': 4386, 'moka': 4387, 'films': 4388, 'uchal': 4389, 'aajkal': 4390, 'needs': 4391, 'gd': 4392, 'abey': 4393, 'bataye': 4394, 'chaye': 4395, 'drop': 4396, 'market': 4397, 'works': 4398, 'shab': 4399, 'evil': 4400, 'improve': 4401, 'upside-down_face': 4402, 'heavy_heart_exclamation': 4403, 'dancing': 4404, 'near': 4405, 'karu': 4406, 'samajhte': 4407, 'aree': 4408, 'mgr': 4409, 'paya': 4410, 'staff': 4411, 'sunta': 4412, 'putting': 4413, 'facts': 4414, 'bike': 4415, 'jae': 4416, 'nerd_face': 4417, 'sabne': 4418, 'majority': 4419, 'rafale': 4420, 'cheers': 4421, 'kurta': 4422, 'network': 4423, 'insane': 4424, 'kpk': 4425, 'baji': 4426, 'chairman': 4427, 'khali': 4428, 'mandate': 4429, 'gujju': 4430, 'loge': 4431, 'duty': 4432, 'jalwa': 4433, 'verma': 4434, 'hamre': 4435, 'netao': 4436, 'before': 4437, '15daystobharat': 4438, 'pae': 4439, 'everyday': 4440, 'vision': 4441, 'bijli': 4442, 'aisy': 4443, 'thaa': 4444, 'uwu': 4445, 'trying': 4446, 'mangne': 4447, 'bhikhari': 4448, 'maryam': 4449, 'ambani': 4450, 'beizzati': 4451, 'awam': 4452, 'gaan': 4453, 'oic': 4454, 'buhat': 4455, 'islamic': 4456, '@': 4457, 'amarinder': 4458, 'issue': 4459, 'kadam': 4460, 'rakhta': 4461, 'sunn': 4462, 'interesting': 4463, 'streaming': 4464, 'strength': 4465, 'working': 4466, 'jeetega': 4467, 'engvsa': 4468, 'tun': 4469, 'sneezing_face': 4470, 'showing': 4471, 'smart': 4472, 'phone': 4473, 'marta': 4474, 'rofique': 4475, 'kisse': 4476, 'vikash': 4477, 'bich': 4478, '26': 4479, 'khair': 4480, 'sooch': 4481, 'payment': 4482, 'bikau': 4483, 'extremely': 4484, 'banai': 4485, 'tco': 4486, 'black': 4487, 'peene': 4488, 'lein': 4489, 'dikhai': 4490, 'kau': 4491, 'wave24': 4492, 'muddo': 4493, 'khade': 4494, 'anurodh': 4495, 'hazrat': 4496, 'hr': 4497, 'wqt': 4498, 'akele': 4499, 'kursi': 4500, 'chaudhary': 4501, 'masjid': 4502, 'dakho': 4503, 'krain': 4504, 'degree': 4505, 'folded_hands_medium-light_skin_tone': 4506, 'bangal': 4507, 'sbko': 4508, 'chud': 4509, 'youth': 4510, 'bhagat': 4511, 'rhegi': 4512, 'fast': 4513, 'hahah': 4514, 'gaaon': 4515, 'bolenge': 4516, 'bhoot': 4517, 'hashtag': 4518, 'spoke': 4519, 'tyagi': 4520, 'question': 4521, 'bhadve': 4522, 'kahani': 4523, 'laughing': 4524, 'quality': 4525, 'debates': 4526, 'maari': 4527, 'upcoming': 4528, 'balloon': 4529, 'nakko': 4530, 'dhandha': 4531, 'dimak': 4532, 'watan': 4533, 'kejri': 4534, 'galt': 4535, 'rh': 4536, 'malik': 4537, 'stop': 4538, 'repeat': 4539, 'pra': 4540, 'paji': 4541, 'sardar': 4542, 'company': 4543, 'jhut': 4544, 'ec': 4545, 'leta': 4546, 'hmlog': 4547, 'advice': 4548, 'phla': 4549, 'ain': 4550, 'lying': 4551, 'wickets': 4552, 'ullu': 4553, 'three': 4554, 'jaroorat': 4555, 'laanat': 4556, 'uda': 4557, 'baarik': 4558, 'nirnay': 4559, 'rahy': 4560, 'mullah': 4561, 'yai': 4562, 'aaaj': 4563, 'online': 4564, 'website': 4565, 'pictures': 4566, 'umrah': 4567, 'esi': 4568, 'gaana': 4569, 'hazaro': 4570, 'pistol': 4571, 'yaqeen': 4572, 'der': 4573, 'hoy': 4574, '#‚Ä¶': 4575, 'indies': 4576, 'toss': 4577, 'thee': 4578, 'bhakti': 4579, '24': 4580, 'sache': 4581, 'usa': 4582, 'middle': 4583, 'electionresults2019': 4584, 'sada': 4585, 'hari': 4586, 'jii': 4587, 'tl': 4588, 'ganda': 4589, 'sr': 4590, 'kahta': 4591, 'mi': 4592, 'wha': 4593, 'petrol': 4594, 'zabardast': 4595, 'honest': 4596, 'likho': 4597, 'muslimo': 4598, 'dee': 4599, 'muze': 4600, 'ghor': 4601, 'jita': 4602, 'persons': 4603, 'liked': 4604, 'ending': 4605, 'towards': 4606, 'pat': 4607, 'jatey': 4608, 'ramazan': 4609, \"we've\": 4610, 'rishta': 4611, 'research': 4612, 'sonch': 4613, 'jio': 4614, 'promotions': 4615, 'partner': 4616, 'wanted': 4617, 'desi': 4618, 'secret': 4619, 'kaisi': 4620, 'kali': 4621, 'andhi': 4622, 'male': 4623, 'contact': 4624, 'uppsc': 4625, 'khule': 4626, 'bik': 4627, 'cheap': 4628, 'born': 4629, 'anand': 4630, 'bai': 4631, 'salaam': 4632, 'lucknow': 4633, 'update': 4634, 'congratulate': 4635, 'victim': 4636, 'chlega': 4637, 'dho': 4638, 'chhota': 4639, 'defence': 4640, 'chahate': 4641, 'jalan': 4642, 'khila': 4643, 'happening': 4644, 'laakh': 4645, 'downcast_face_with_sweat': 4646, 'food': 4647, 'phele': 4648, 'kiu': 4649, 'gobar': 4650, 'joh': 4651, 'invite': 4652, 'earn': 4653, 'bap': 4654, 'hmari': 4655, 'bhenchod': 4656, 'jindgi': 4657, 'za': 4658, 'club': 4659, 'kesi': 4660, 'itana': 4661, 'mansik': 4662, 'humesa': 4663, 'salamat': 4664, 'zarorat': 4665, 'kaya': 4666, 'natak': 4667, 'hara': 4668, 'chutye': 4669, 'abhiyan': 4670, 'karnataka': 4671, 'azhar': 4672, 'ep': 4673, 'nigam': 4674, 'massive': 4675, 'attack': 4676, 'chahie': 4677, 'paki': 4678, 'stars': 4679, 'gau': 4680, 'buddy': 4681, 'koshish': 4682, 'mea': 4683, 'jam': 4684, 'gaddaro': 4685, 'thekedar': 4686, 'sidhe': 4687, 'hindustani': 4688, 'cheating': 4689, 'bula': 4690, 'else': 4691, 'cricket_game': 4692, 'hd': 4693, 'indira': 4694, 'happen': 4695, 'hooo': 4696, 'dikhana': 4697, 'anna': 4698, 'musical_score': 4699, 'woman_dancing': 4700, 'visit': 4701, 'banti': 4702, 'boli': 4703, 'alaikum': 4704, 'nature': 4705, 'kl': 4706, 'paye': 4707, 'kavi': 4708, 'chalana': 4709, 'minute': 4710, 'kion': 4711, 'padhte': 4712, 'jyadatar': 4713, 'chatne': 4714, 'kri': 4715, 'grow': 4716, 'heart_decoration': 4717, 'bibi': 4718, 'id': 4719, 'mulla': 4720, 'hearty': 4721, 'reach': 4722, 'payal': 4723, 'pandit': 4724, 'blame': 4725, 'solution': 4726, 'transmission': 4727, 'energy': 4728, 'dizzy': 4729, 'pagla': 4730, 'achhi': 4731, 'molana': 4732, 'kareem': 4733, 'lenge': 4734, 'khujli': 4735, 'jinhone': 4736, 'ley': 4737, 'forgive': 4738, 'kaan': 4739, 'seems': 4740, 'harkatein': 4741, 'elected': 4742, 'badnaam': 4743, 'boyfriend': 4744, 'guzar': 4745, 'kraaa': 4746, 'move': 4747, 'barabri': 4748, '60': 4749, 'anupam': 4750, 'kher': 4751, 'achchi': 4752, 'tamanna': 4753, 'nikali': 4754, 'nivedan': 4755, 'khoj': 4756, 'nuksaan': 4757, 'madar': 4758, 'moron': 4759, 'feminism': 4760, 'cat': 4761, 'mindset': 4762, 'kharid': 4763, 'liking': 4764, 'puchne': 4765, 'jagha': 4766, 'chalte': 4767, 'cover': 4768, '25': 4769, 'books': 4770, 'bahr': 4771, 'breaking': 4772, 'pur': 4773, 'bachha': 4774, 'gf': 4775, 'kary': 4776, 'alive': 4777, 'blossom': 4778, 'tumlogo': 4779, 'ramdan': 4780, 'bowler': 4781, 'ias': 4782, 'jdu': 4783, 'amar': 4784, 'lund': 4785, 'bolega': 4786, 'young': 4787, 'especially': 4788, 'hamary': 4789, 'goals': 4790, 'hahahah': 4791, 'nich': 4792, 'ce': 4793, 'imandaar': 4794, 'ako': 4795, 'presence': 4796, 'rahay': 4797, 'jante': 4798, 'ri': 4799, 'kyonki': 4800, 'namastey': 4801, 'manik': 4802, 'dream': 4803, 'mulle': 4804, 'niti': 4805, 'hyderabad': 4806, 'illegal': 4807, 'blessings': 4808, 'moments': 4809, 'anymore': 4810, 'ng': 4811, 'turn': 4812, 'shaks': 4813, 'bachche': 4814, 'neend': 4815, 'bharti': 4816, 'likhe': 4817, 'aan': 4818, 'khate': 4819, 'pit': 4820, 'usay': 4821, 'lana': 4822, 'mjy': 4823, 'likes': 4824, '((': 4825, 'aankhe': 4826, 'suraj': 4827, 'bechare': 4828, 'bjp4india': 4829, 'amitshah': 4830, 'halala': 4831, 'peda': 4832, 'key': 4833, 'bhaii': 4834, 'alka': 4835, 'race': 4836, 'mrng': 4837, 'sonytv': 4838, 'fantastic': 4839, 'killed': 4840, 'rote': 4841, 'haste': 4842, 'head': 4843, 'face_with_raised_eyebrow': 4844, 'jinda': 4845, 'dhruv': 4846, 'contest': 4847, 'rakhte': 4848, 'jagan': 4849, 'surely': 4850, 'bakra': 4851, 'netaji': 4852, 'jesi': 4853, 'dtc': 4854, 'lg': 4855, 'trh': 4856, 'windies': 4857, 'rank': 4858, 'sent': 4859, 'everytime': 4860, 'pun': 4861, 'gande': 4862, 'awards': 4863, 'bhaiyya': 4864, 'pajama': 4865, 'tamil': 4866, 'launch': 4867, 'mare': 4868, 'khaye': 4869, 'product': 4870, 'arjun': 4871, 'scared': 4872, 'beshak': 4873, 'nikhil': 4874, \"i've\": 4875, 'analysis': 4876, 'tired_face': 4877, 'wonder': 4878, 'jimmedar': 4879, 'running': 4880, 'abbas': 4881, 'folded_hands_medium_skin_tone': 4882, 'singer': 4883, 'bhara': 4884, 'wicket': 4885, 'aasha': 4886, '√•': 4887, 'nalayak': 4888, 'journalism': 4889, 'ben': 4890, 'yahe': 4891, 'district': 4892, 'bhaya': 4893, 'jitoge': 4894, 'jaison': 4895, 'teray': 4896, 'chez': 4897, 'lagaya': 4898, 'zeeshan09': 4899, 'izat': 4900, 'kosis': 4901, 'khyal': 4902, 'brilliant': 4903, 'pehan': 4904, 'chahiy': 4905, 'responsible': 4906, 'rahiye': 4907, 'tukde': 4908, '????': 4909, 'promo': 4910, 'non': 4911, 'dimaag': 4912, 'sth': 4913, 'chaat': 4914, 'disaster': 4915, 'whatever': 4916, 'fandom': 4917, 'dill': 4918, 'pehchan': 4919, 'sarangi': 4920, 'ajinkya': 4921, 'rahane': 4922, 'karle': 4923, 'champions': 4924, 'soo': 4925, 'shapath': 4926, 'lala': 4927, 'khubsurat': 4928, 'shak': 4929, 'zindabaad': 4930, 'ganna': 4931, 'apn': 4932, 'jaayega': 4933, 'angry': 4934, 'dhoop': 4935, '56': 4936, 'lungi': 4937, \"let's\": 4938, 'jaega': 4939, 'choose': 4940, 'scenes': 4941, 'traffic': 4942, 'dogle': 4943, 'dekhlo': 4944, 'brown': 4945, 'test': 4946, 'snapchat': 4947, 'frnd': 4948, '@@': 4949, 'feb': 4950, 'lee': 4951, '..!': 4952, 'prerna': 4953, 'pille': 4954, 'third': 4955, 'woman_facepalming': 4956, 'sehmat': 4957, 'gone': 4958, 'mad': 4959, 'barat': 4960, 'tukre': 4961, 'karane': 4962, 'sadhvi': 4963, 'pragya': 4964, 'brand': 4965, 'saray': 4966, 'smriti': 4967, 'hans': 4968, 'kamini': 4969, 'military': 4970, '?????': 4971, 'ana': 4972, 'strike': 4973, 'anjana': 4974, 'thok': 4975, 'banna': 4976, 'kate': 4977, 'received': 4978, 'expect': 4979, 'kari': 4980, 'inch': 4981, 'bangla': 4982, 'aray': 4983, 'bahoot': 4984, 'hotay': 4985, 'ninda': 4986, 'abhisar': 4987, 'karney': 4988, \"you'll\": 4989, 'masti': 4990, 'huwa': 4991, 'jaisay': 4992, 'count': 4993, 'vha': 4994, 'tmhare': 4995, 'rashtra': 4996, 'iron': 4997, 'das': 4998, 'bnega': 4999, 'facebook': 5000, 'jaata': 5001, 'gain': 5002, 'paidmediacantharmsalman': 5003, 'children': 5004, 'rning': 5005, 'tumari': 5006, 'shaam': 5007, 'mate': 5008, 'honey': 5009, 'garibon': 5010, 'khusi': 5011, 'rahta': 5012, 'aisha': 5013, 'gir': 5014, 'bestie': 5015, 'ginti': 5016, 'gandagi': 5017, 'kanhi': 5018, 'edit': 5019, 'insta': 5020, 'sehat': 5021, 'stan': 5022, 'irada': 5023, 'blue': 5024, 'tamacha': 5025, 'kren': 5026, 'term': 5027, 'tik': 5028, 'tok': 5029, '72': 5030, 'deal': 5031, 'hye': 5032, 'samarthan': 5033, 'mehanga': 5034, 'aukad': 5035, 'vijay': 5036, 'sunne': 5037, 'jat': 5038, 'complete': 5039, 'paani': 5040, 'molvi': 5041, 'huh': 5042, 'jeene': 5043, '52': 5044, 'winner': 5045, 'krt': 5046, 'artist': 5047, 'window': 5048, 'krke': 5049, 'vishvas': 5050, 'billi': 5051, 'jokes': 5052, 'legi': 5053, 'http': 5054, 'vibhag': 5055, 'khaya': 5056, 'international': 5057, 'assam': 5058, 'betiyon': 5059, 'abp': 5060, 'ney': 5061, 'answered': 5062, 'bahat': 5063, 'bharatiya': 5064, 'paar': 5065, 'nagrik': 5066, 'jain': 5067, 'chuna': 5068, 'water': 5069, 'usy': 5070, 'boo': 5071, 'ysjagan': 5072, 'reddy': 5073, 'meerut': 5074, 'shukr': 5075, 'kyse': 5076, 'kolkata': 5077, 'loud': 5078, 'azeem': 5079, 'behind': 5080, 'interested': 5081, 'andhra': 5082, 'andhere': 5083, 'kamina': 5084, 'horaha': 5085, 'jija': 5086, 'pahli': 5087, 'wedding': 5088, 'paap': 5089, 'sree': 5090, 'teams': 5091, 'bahana': 5092, 'badh': 5093, 'telangana': 5094, 'ground': 5095, 'pare': 5096, 'anil': 5097, 'factory': 5098, 'zindgi': 5099, 'farma': 5100, 'prize': 5101, 'tarika': 5102, 'mamataofficial': 5103, 'lokpriya': 5104, 'lip': 5105, 'banke': 5106, 'koti': 5107, 'shabash': 5108, 'dimagh': 5109, 'indeed': 5110, 'snake': 5111, 'pair': 5112, 'took': 5113, 'golden': 5114, 'workshop': 5115, '11': 5116, 'afghanistan': 5117, 'hmm': 5118, 'sana': 5119, 'zartaj': 5120, 'aaega': 5121, 'roza': 5122, 'ghalti': 5123, 'araha': 5124, 'padegi': 5125, 'khe': 5126, 'miyan': 5127, 'zeal': 5128, 'suprb': 5129, 'lets': 5130, 'khatarnak': 5131, 'kinds': 5132, 'kanjar': 5133, '≈ü': 5134, 'chhor': 5135, 'pichwade': 5136, 'virat': 5137, 'lgti': 5138, 'karva': 5139, 'cast': 5140, 'dhol': 5141, 'btaye': 5142, 'laa': 5143, 'sending': 5144, 'qadri': 5145, 'kama': 5146, 'midiya': 5147, 'kharap': 5148, 'lfc': 5149, 'karunga': 5150, 'chata': 5151, 'learn': 5152, 'hii': 5153, 'nana': 5154, 'silent': 5155, 'palms_up_together': 5156, 'saf': 5157, 'wasn': 5158, 'sleeping': 5159, 'nab': 5160, 'express': 5161, 'bed': 5162, 'bachcha': 5163, '2011': 5164, 'aksar': 5165, 'bhasan': 5166, 'rocks': 5167, 'comedian': 5168, 'blowing': 5169, 'sapno': 5170, '90': 5171, 'shabd': 5172, 'loyal': 5173, 'compare': 5174, 'nahe': 5175, 'global': 5176, 'ambedkar': 5177, 'shriman': 5178, 'mouth': 5179, 'bechne': 5180, 'ussi': 5181, 'wb': 5182, 'khiladi': 5183, 'fall': 5184, 'parties': 5185, '2015': 5186, 'soche': 5187, 'nepotism': 5188, 'giveaway': 5189, 'dosra': 5190, 'humlog': 5191, 'mager': 5192, 'computer': 5193, 'simrn': 5194, 'aww': 5195, 'sooooo': 5196, 'dialogue': 5197, 'spurs': 5198, 'sanjeev': 5199, 'except': 5200, 'truly': 5201, 'fizakhan': 5202, 'aaram': 5203, 'insaniyat': 5204, 'initiative': 5205, 'itihas': 5206, 'rhega': 5207, 'safdar': 5208, 'kutti': 5209, 'gurmeetramrahim': 5210, 'visvash': 5211, 'bany': 5212, 'shaa': 5213, 'krwa': 5214, 'danda': 5215, 'allahumma': 5216, 'khatra': 5217, 'prince': 5218, 'aadarniya': 5219, 'mani': 5220, 'chhoti': 5221, 'customer': 5222, 'stuff': 5223, 'rota': 5224, 'fought': 5225, 'fill': 5226, 'samajhdar': 5227, 'empty': 5228, 'expression': 5229, 'mjhy': 5230, 'riyasat': 5231, 'catch': 5232, 'sabr': 5233, 'safai': 5234, 'follower': 5235, 'yho': 5236, 'situation': 5237, 'vaiya': 5238, 'junmyeon': 5239, 'budday': 5240, 'giya': 5241, 'moon': 5242, 'whenever': 5243, 'gn': 5244, 'doosra': 5245, 'sometimes': 5246, 'pag': 5247, 'yayyyyyy': 5248, 'gorgeous': 5249, 'dulhan': 5250, 'marriage': 5251, 'nose': 5252, 'dharmantaran': 5253, 'otherwise': 5254, 'adiza': 5255, 'gaadi': 5256, 'yay': 5257, 'bloody': 5258, 'utar': 5259, 'sher': 5260, 'samasya': 5261, 'kand': 5262, 'joote': 5263, 'asif': 5264, 'ajkal': 5265, 'aparshakti': 5266, 'tujhy': 5267, 'kiun': 5268, 'jatay': 5269, 'anpadh': 5270, 'res': 5271, 'raam': 5272, 'safal': 5273, 'nyay': 5274, 'republic': 5275, 'disha': 5276, 'points': 5277, 'needed': 5278, 'sc': 5279, 'suspend': 5280, 'fek': 5281, 'rishte': 5282, 'ashish': 5283, 'talwe': 5284, 'godse': 5285, 'pichhle': 5286, 'ball': 5287, 'dick': 5288, 'sochte': 5289, 'shrm': 5290, 'pile_of_poo': 5291, 'seedling': 5292, 'forces': 5293, 'face_with_symbols_on_mouth': 5294, 'tari': 5295, 'aaja': 5296, 'sound': 5297, 'aagaya': 5298, 'temple': 5299, 'jakar': 5300, 'debut': 5301, 'jaante': 5302, 'bevkuf': 5303, 'tn': 5304, 'shankar': 5305, 'moo': 5306, 'cricketworldcup': 5307, 'butterfly': 5308, 'sushmaswaraj': 5309, 'right_arrow': 5310, 'fatima': 5311, 'budhi': 5312, '****': 5313, 'phale': 5314, 'varanasi': 5315, 'bikaner': 5316, 'khaaya': 5317, 'mano': 5318, 'nitin': 5319, 'hand_with_fingers_splayed_medium-light_skin_tone': 5320, 'jus': 5321, 'smritiirani': 5322, 'myogiadityanath': 5323, 'incindia': 5324, 'tejasvi': 5325, 'asia': 5326, 'kavita': 5327, 'kii': 5328, 'wasiyo': 5329, 'hony': 5330, 'derasachasauda': 5331, 'fawadchaudhry': 5332, 'manei': 5333, 'registered': 5334, 'rubikaliyaquat': 5335, 'chetra': 5336, 'bhejna': 5337, 'jy': 5338, '_': 5339, '√∞√øÀú‚Ä¶': 5340, '√¢‚Ç¨¬¶': 5341, 'https': 5342, 'lambaalka': 5343, 'arvindkejriwal': 5344, '√¢‚Ç¨‚Äù': 5345, 'tarekfatah': 5346, '√∞√ø‚Ä°¬≥': 5347, 'jhasanjay': 5348, '√∞√ø‚Ä°': 5349, '√∞√ø‚Ä°¬≥√∞√ø‚Ä°': 5350, 'maryamnsharif': 5351, 'priyankagandhi': 5352, 'sardanarohit': 5353, 'kunalkamra88': 5354, 'sirpareshrawal': 5355, 'imrankhanpti': 5356, '√¢': 5357, 'rssurjewala': 5358, '√¢‚Ç¨': 5359, '√¢\\x9d¬§√Ø¬∏\\x8f': 5360, '√∞√ø': 5361, '\\x8f√∞√ø': 5362, '\\x8f': 5363, '√¢‚Ç¨≈ì': 5364, '√¢‚Ç¨\\x9d': 5365, 'waseembadami': 5366, 'pmoindia': 5367, 'iamsrk': 5368, 'asadowaisi': 5369, 'yadavakhilesh': 5370, 'beingsalmankhan': 5371, '√∞√øÀú≈°': 5372, '√¢‚Ç¨Àú': 5373, 'ashutosh83b': 5374, '\\x8f√∞√ø\\x8f¬ª': 5375, 'srbachchan': 5376, 'manakgupta': 5377, '√∞√øÀú‚Äö': 5378, 'anjanaomkashyap': 5379, 'ranaayyub': 5380, 'aamaadmiparty': 5381, '√∞√ø≈æ‚Ä∞': 5382, 'tajinderbagga': 5383, 'sherryontopp': 5384, '.√¢‚Ç¨¬¶': 5385, '√∞√ø¬§‚Äù': 5386, 'kamaalrkhan': 5387, 'drkumarvishwas': 5388, 'piyushgoyal': 5389, 'mlkhattar': 5390, '√¢√ø¬∂': 5391, 'sanjayazadsln': 5392, 'swamy39': 5393, 'acharyapramodk': 5394, '√∞√ø‚Äò\\x8d': 5395, '√∞√ø¬§¬£√∞√ø¬§¬£√∞√ø¬§¬£': 5396, 'kapilsharmak9': 5397, '√∞√ø≈ì¬π': 5398, '\\x8f√∞√ø\\x8f¬ª√∞√ø': 5399, 'hardikpatel': 5400, 'akshaykumar': 5401, '‚Äö': 5402, 'sardesairajdeep': 5403, '√∞√øÀú‚Äö√∞√øÀú‚Äö': 5404, 'bbhuttozardari': 5405, 'vikasbha': 5406, 'abpnewstv': 5407, '√¢\\x9d¬§': 5408, '‚Äû': 5409, '√∞√ø‚Äô‚Ä¢': 5410, 'officeofknath': 5411, '√∞√øÀú‚Äö√∞√øÀú‚Äö√∞√øÀú‚Äö': 5412, 'msisodia': 5413, 'gautamgambhir': 5414, 'rsprasad': 5415, '√¢≈ì‚Ä¶': 5416, 'reallyswara': 5417, '√∞√øÀú\\xad': 5418, '√∞√øÀú': 5419, 'kanhaiyakumar': 5420, 'sarfaraza': 5421, 'waglenikhil': 5422}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A193riIE-6KF",
    "outputId": "369164c4-c237-48c9-cd10-fa044286be02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p=len(list(word_to_id.keys()))\n",
    "print(p)\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm2HU4sX-7z9"
   },
   "outputs": [],
   "source": [
    "def summarize_data():\n",
    "  \"\"\"\n",
    "  Output:\n",
    "                    classes: list, list of unique classes in y\n",
    "                no_of_words: int, number of unique words in dataset x\n",
    "     list_of_review_lengths: list,  list of lengths of each review\n",
    "         mean_review_length: float, mean(list_of_review_lengths), a single floating point value\n",
    "          std_review_length: float, standard_deviation(list_of_review_lengths), a single floating point value\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  classes=(np.unique(y))\n",
    "  no_of_words=len(v)\n",
    "  list_of_review_lengths=[]\n",
    "  for i in X:\n",
    "    list_of_review_lengths.append(len(i))\n",
    "  mean_review_length=np.mean(list_of_review_lengths)\n",
    "  std_review_length=np.std(list_of_review_lengths)\n",
    "\n",
    "\n",
    "\n",
    "  return classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length\n",
    "\n",
    "\n",
    "classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length = summarize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h62M5gKv_VYT",
    "outputId": "46d7111f-e8a4-4e3b-e37d-d1bb3c7f7a12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ZuDvomj_YQk",
    "outputId": "54e2407e-7492-4f65-e725-742a933ae477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "-7KSAjl5_aBZ",
    "outputId": "ea1f2d5b-db80-41af-809a-4b5defe0f4af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid asi takhle vypad √° m ≈Ø j life .',\n",
       "       'haan yaar neha pensive_face pensive_face kab karega woh post loudly_crying_face usne na sach mein photoshoot karna chahiye phir woh post karega .',\n",
       "       'television media congress ke liye nhi h . ye toh aapko pata chal hi gya hoga . achha hoga ki congress ke .',\n",
       "       ...,\n",
       "       'rt aap logo ki baat nahi kar raha najim bhai media walo ki kar raha hu jo bina janche parkhe one sided news .',\n",
       "       '__ jay jay shree ram ram rajye aaya he danavo rakshasho chudelno or surpankha ke gharoke sa .',\n",
       "       'mujhe to bhayankar ho gaya hai slightly_smiling_face shayad ab mai sahi aur galat bhi samajhna nahi chahta . mujhe kisi .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    y: numpy array with class labels\n",
    "  Outputs:\n",
    "    y_oh: numpy array with corresponding one-hot encodings\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  y_oh=[]\n",
    "  for i in y:\n",
    "    if i==0:\n",
    "      y_oh.append([1,0])\n",
    "    else:\n",
    "      y_oh.append([0,1])\n",
    "  y_oh=np.array(y_oh)\n",
    "  return y_oh\n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J1Nj6II6_cda",
    "outputId": "e6231b50-8c3f-4077-f255-4d2c7daf72d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5423"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "B1RacIec_epR",
    "outputId": "701a1b11-eb75-4c20-ee71-fa198f9e901e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train  (14000, 15465)\n",
      "x_test  (3000, 15465)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_hot_encode(sequences, dimension):\n",
    "  \"\"\"\n",
    "    Input:\n",
    "          sequences: list of sequences in X_train or X_test\n",
    "    Output:\n",
    "          results: mult numpy matrix of shape(len(sequences), dimension)\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  result=[]\n",
    "  for i in sequences:\n",
    "    \n",
    "    g=[0]*dimension\n",
    "    for h in i.split(\" \"):\n",
    "      if(h in word_to_id.keys()):\n",
    "        g[word_to_id[h]]=1\n",
    "    result.append(g)\n",
    "  result=np.array(result)\n",
    "  return result\n",
    "\n",
    "x_train = multi_hot_encode(X_train, 15465)\n",
    "x_test = multi_hot_encode(X_test, 15465)\n",
    "\n",
    "print(\"x_train \", x_train.shape)\n",
    "print(\"x_test \", x_test.shape)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "3mYlhK4H_iOB",
    "outputId": "7a526dd7-2dab-4dba-bf35-b0d02346057b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 7000)              108262000 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3000)              21003000  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              3001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 132,268,002\n",
      "Trainable params: 132,268,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8400 samples, validate on 5600 samples\n",
      "Epoch 1/4\n",
      "8400/8400 [==============================] - 11s 1ms/step - loss: 1.0140 - accuracy: 0.5786 - val_loss: 0.6638 - val_accuracy: 0.6239\n",
      "Epoch 2/4\n",
      "8400/8400 [==============================] - 9s 1ms/step - loss: 0.6755 - accuracy: 0.6045 - val_loss: 0.6682 - val_accuracy: 0.6239\n",
      "Epoch 3/4\n",
      "8400/8400 [==============================] - 9s 1ms/step - loss: 0.6703 - accuracy: 0.6240 - val_loss: 0.6693 - val_accuracy: 0.6239\n",
      "Epoch 4/4\n",
      "8400/8400 [==============================] - 9s 1ms/step - loss: 0.6667 - accuracy: 0.6240 - val_loss: 0.6661 - val_accuracy: 0.6239\n",
      "Accuracy of your model is\n",
      "63.333332538604736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0139633764823277,\n",
       " 0.6755127012729645,\n",
       " 0.6703494936227798,\n",
       " 0.6666945914427439]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_strat, x_dev, y_strat, y_dev = train_test_split(x_train, y_train,test_size=0.40,random_state=0, stratify=y_train)\n",
    "x_strat\n",
    "\n",
    "\"\"\"## Build Model\n",
    "Build a multi layered feed forward network in keras.\n",
    "### Create the model\n",
    "\"\"\"\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        model: A compiled keras model\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model=Sequential()\n",
    "    model.add(Dense(7000,input_dim=x_strat.shape[1],activation='sigmoid'))\n",
    "    model.add(Dense(3000,activation='sigmoid'))\n",
    "    model.add(Dense(1000,activation='sigmoid'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"### Fit the Model\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def fit(model):\n",
    "    \"\"\"\n",
    "    Action:\n",
    "        Fit the model created above using training data as x_strat and y_strat\n",
    "        and validation_data as x_dev and y_dev, verbose=2 and store it in 'history' variable.\n",
    "        evaluate the model using x_test, y_test, verbose=0 and store it in 'scores' list\n",
    "    Output:\n",
    "        scores: list of length 2\n",
    "        history_dict: output of history.history where history is output of model.fit()\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    history=model.fit(x_strat, y_strat, epochs=4, batch_size=100,validation_data=(x_dev,y_dev), verbose=1)\n",
    "    history_dict=history.history\n",
    "    scores=model.evaluate(x_test,y_test,verbose=0)\n",
    "    return scores,history_dict\n",
    "\n",
    "\n",
    "scores,history_dict = fit(model)\n",
    "\n",
    "Accuracy=scores[1]*100\n",
    "print('Accuracy of your model is')\n",
    "print(scores[1]*100)\n",
    "\n",
    "history_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "i5tLLLDZ_m1U",
    "outputId": "276a0afd-3e1a-4ec5-a88f-4dee617d7060"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, (len(history_dict['loss']) + 1))\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "gNmRNKV6AsJ3",
    "outputId": "b9eab930-2e3f-4552-f891-67eb443dbe2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.57857144, 0.60452384, 0.62404764, 0.62404764],\n",
       " 'loss': [1.0139633764823277,\n",
       "  0.6755127012729645,\n",
       "  0.6703494936227798,\n",
       "  0.6666945914427439],\n",
       " 'val_accuracy': [0.6239285469055176,\n",
       "  0.6239285469055176,\n",
       "  0.6239285469055176,\n",
       "  0.6239285469055176],\n",
       " 'val_loss': [0.6637579575181007,\n",
       "  0.6682095815028463,\n",
       "  0.6692597748977798,\n",
       "  0.6661173030734062]}"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "MJTYRxCCAuND",
    "outputId": "b0b87c4b-f1de-40a8-db0b-3876dc79d3b3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, (len(history_dict['accuracy']) + 1))\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIchDP8HAw5Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJLQg46CBINu"
   },
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdZCq7dDBM_J"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "pml5JlcXBPK9",
    "outputId": "bc5a3a61-7935-4847-e46c-a887862c38f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid asi takhle vypad √° m ≈Ø j life .',\n",
       "       'haan yaar neha pensive_face pensive_face kab karega woh post loudly_crying_face usne na sach mein photoshoot karna chahiye phir woh post karega .',\n",
       "       'television media congress ke liye nhi h . ye toh aapko pata chal hi gya hoga . achha hoga ki congress ke .',\n",
       "       ...,\n",
       "       'rt aap logo ki baat nahi kar raha najim bhai media walo ki kar raha hu jo bina janche parkhe one sided news .',\n",
       "       '__ jay jay shree ram ram rajye aaya he danavo rakshasho chudelno or surpankha ke gharoke sa .',\n",
       "       'mujhe to bhayankar ho gaya hai slightly_smiling_face shayad ab mai sahi aur galat bhi samajhna nahi chahta . mujhe kisi .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8zw8kJDBQ95"
   },
   "outputs": [],
   "source": [
    "test_tweets=np.array(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyNE4vexBTWo"
   },
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "XIRXtohYBVOL",
    "outputId": "d0a5ebaf-4fbd-4ab7-d119-b00a996b0e4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid asi takhle vypad √° m ≈Ø j life .',\n",
       "       'haan yaar neha pensive_face pensive_face kab karega woh post loudly_crying_face usne na sach mein photoshoot karna chahiye phir woh post karega .',\n",
       "       'television media congress ke liye nhi h . ye toh aapko pata chal hi gya hoga . achha hoga ki congress ke .',\n",
       "       ...,\n",
       "       'rt aap logo ki baat nahi kar raha najim bhai media walo ki kar raha hu jo bina janche parkhe one sided news .',\n",
       "       '__ jay jay shree ram ram rajye aaya he danavo rakshasho chudelno or surpankha ke gharoke sa .',\n",
       "       'mujhe to bhayankar ho gaya hai slightly_smiling_face shayad ab mai sahi aur galat bhi samajhna nahi chahta . mujhe kisi .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets'].values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtLU3ylCBW5b"
   },
   "outputs": [],
   "source": [
    "for i in test_tweets:\n",
    "  clean_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2VHVLmxBej0"
   },
   "outputs": [],
   "source": [
    "frames = [df['tweets'],pd.DataFrame(test_tweets)]\n",
    "result = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8puTtKkBjdV"
   },
   "outputs": [],
   "source": [
    "X=np.hstack((df['tweets'].values,test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBbN5st4BlFR"
   },
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2BwfrzdgBm0i",
    "outputId": "10031d60-97fc-405f-d250-af8e8f4fdad2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000, 116)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1UpBVlwBo9r"
   },
   "outputs": [],
   "source": [
    "testtweets=X[14000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-9yNotz8BrMz",
    "outputId": "5705dc78-2e71-458a-8c2a-95dfa22ef69f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testtweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwd8qjOlBslZ"
   },
   "outputs": [],
   "source": [
    "X=X[:14000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "d5qH0s3IBvMA",
    "outputId": "64dffc9d-691e-48a6-ff32-0b5af3d98a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 116, 128)          256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 116, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Gn1e8KZ1Bw0b",
    "outputId": "21bea930-caa5-43d5-da47-35ec9cd3f31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9380, 116) (9380, 3)\n",
      "(4620, 116) (4620, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "FtWOM4i0BygE",
    "outputId": "a9a9f212-38b0-470f-f67b-a111a9cab247"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 68s - loss: 0.9452 - accuracy: 0.5286\n",
      "Epoch 2/4\n",
      " - 67s - loss: 0.8315 - accuracy: 0.6204\n",
      "Epoch 3/4\n",
      " - 67s - loss: 0.7712 - accuracy: 0.6541\n",
      "Epoch 4/4\n",
      " - 67s - loss: 0.7211 - accuracy: 0.6892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f334e06f908>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 4, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyzAMZF5B0eM"
   },
   "outputs": [],
   "source": [
    "output=[]\n",
    "for x in range(len(testtweets)):\n",
    "    result = model.predict(testtweets[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    output.append(np.argmax(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZL1Os-L5B2D4"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SE5wcYamB6QM",
    "outputId": "b54cfc95-0dc5-4f4d-fa41-7947f2592979"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.43333333333333"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(testlabels['Sentiment'],output)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waBBK07kF7wr"
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732,
     "referenced_widgets": [
      "c3ae0d87308542e29bdedabb8ae48b19",
      "49256d6694284f53a05a4cf914e8c7b2",
      "b22b466a66a84297b75109c83713f05f",
      "7ba2ade06e6745babc9308504dcdf06d",
      "c11e6bb7c9ee4dc383a06220b27370d8",
      "c1c354adca47412796056c5ded6b3367",
      "1d1c5984db67487ea9b36da01d663472",
      "4a75340b61dc41fcb11e64bb1bb3a807"
     ]
    },
    "colab_type": "code",
    "id": "j5tl6GHiB7g9",
    "outputId": "a2b87a20-d021-4de7-a314-3965254ae652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "Found GPU at: /device:GPU:0\n",
      "There are 1 GPU(s) available.\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 778kB 8.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 52.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.0MB 315kB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 37.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f8275994eff3697c212f997b50aeabf210e05b89e6a1f45827fc885aed680d17\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ae0d87308542e29bdedabb8ae48b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "    import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMpOZZ9RGRu3"
   },
   "outputs": [],
   "source": [
    "labels = df['sentiment']\n",
    "sentences=df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "142VhVcrGVTp",
    "outputId": "bb00d6ef-ed1c-45d5-ce5e-a85aeb96a54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ulUaakRGXgN",
    "outputId": "bc55c38c-c084-4964-cffd-5e462e6a798f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx5-TePmGaf-"
   },
   "outputs": [],
   "source": [
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L_gIPnRaGdcm",
    "outputId": "db4aa63f-a8de-4379-f5d5-2a3aa3f88829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  213\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "4fHiTuy3Gfz0",
    "outputId": "46e2a91f-2813-4e78-faac-d0a19944d158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid asi takhle vypad √° m ≈Ø j life .\n",
      "Token IDs: tensor([  101, 11265,  2078,  1037, 25292,  2102,  8945,  4244,  2102,  1058,\n",
      "         2480, 23125, 15488, 10421,  2243,  1062,  8585,  2243,  9808,  3286,\n",
      "         1041,  2439,  2022,  2480, 25389,  1041,  1046,  1037, 17823,  5643,\n",
      "         2278, 15419,  1047, 21273,  2004,  2072, 27006,  7317,  2063,  1058,\n",
      "        22571,  4215,  1037,  1049,  1057,  1046,  2166,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,        \n",
    "                        truncation=True,              # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "label = torch.tensor(labels)\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "PbxQVpfjGh05",
    "outputId": "e1b73adb-705e-48fe-dbc8-f600f1f3deee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  nen √° vist bolest vztek smutek zmatek osam ƒõ lost beznad ƒõ j a nakonec jen klid asi takhle vypad √° m ≈Ø j life .\n",
      "Token IDs: tensor([  101, 11265,  2078,  1037, 25292,  2102,  8945,  4244,  2102,  1058,\n",
      "         2480, 23125, 15488, 10421,  2243,  1062,  8585,  2243,  9808,  3286,\n",
      "         1041,  2439,  2022,  2480, 25389,  1041,  1046,  1037, 17823,  5643,\n",
      "         2278, 15419,  1047, 21273,  2004,  2072, 27006,  7317,  2063,  1058,\n",
      "        22571,  4215,  1037,  1049,  1057,  1046,  2166,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,       \n",
    "                        truncation=True,               # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',\n",
    "                             # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ivqvhi1lGlgl",
    "outputId": "5cb584ca-cd3a-4d80-f70b-9f34a2cf934c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,600 training samples\n",
      "1,400 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kDH6XiJaGotn",
    "outputId": "ed090c51-b0ee-42a6-da64-bdfc8339fd85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 44\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "print(len(train_dataloader),len(validation_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b50648dd6dff4edea1d15b75fc302725",
      "ea6abe68768b40d0b5959372c7431f5c",
      "19a45bf1609b420391e6a1e32663e587",
      "d5138f970ac24380aaf7a78906b9c60d",
      "514bb8ad8ddc4284b4d86ddc5e397d5d",
      "3ec373ea178a4b39bc432b9b50c89b96",
      "de77d628a4454e75ab33637c3bbf450b",
      "1327a10be7434cccbc33f4fe0582da28",
      "48c30f66e8ff476d9295f21fb3988877",
      "d02ad7f151c341ed952aaa6597bcfc1e",
      "d30092e719f04cda94274dfd0e70819f",
      "a7e865f4494843b29ba73851c948fd6d",
      "07f5bc81f9b64e5983cef30295201dec",
      "5d77f362f6354298a8585b6ffe18af45",
      "94f3087e0e614ae281fe6bea3b580bf1",
      "b8b3ad3c079c4c95be8031bf156a8f60"
     ]
    },
    "colab_type": "code",
    "id": "dDu59gEfGq_k",
    "outputId": "c4918a46-b9e0-4c69-e850-14cdc2bc04da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50648dd6dff4edea1d15b75fc302725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c30f66e8ff476d9295f21fb3988877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    \n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JTUuhiMGtPG"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr = 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9jfSZHgGwQJ"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mH41rQbUGyaj"
   },
   "outputs": [],
   "source": [
    "import numpy.random as random\n",
    "seed_val=10\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6BoVjc0G0IU"
   },
   "outputs": [],
   "source": [
    "def get_label(preds):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  return pred_flat \n",
    "def get_accuracy(y_true,y_pred):\n",
    "   return (min(f1_score(y_true, y_pred, average='macro'),f1_score(y_true, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "gTTD9oYRG2MX",
    "outputId": "578390af-8d14-4d91-89cf-45745938f4b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Running the 0 epoch-------\n",
      "Loss For This Epoch is 0.8960106918957028\n",
      "-----Running the 0 epoch. Time Taken is159.41740655899048-------\n",
      "---- Accuracy for this Epoch is 0.4444444444444445\n",
      "-----Running the 1 epoch-------\n",
      "Loss For This Epoch is 0.7534620096235711\n",
      "-----Running the 1 epoch. Time Taken is157.72832608222961-------\n",
      "---- Accuracy for this Epoch is 0.5773993808049536\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "training_stat=[]\n",
    "for epoch in range(0,2):\n",
    "  print(\"-----Running the {} epoch-------\".format(epoch))\n",
    "  t0 = time.time()\n",
    "  train_loss=0\n",
    "  model.train()\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    b_input=batch[0].to('cuda')\n",
    "    b_input_mask=batch[1].to('cuda')\n",
    "    y=batch[2].to('cuda')\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss,output=model(b_input,\n",
    "                      attention_mask=b_input_mask,\n",
    "                      labels=y,\n",
    "                      token_type_ids=None        \n",
    "    )\n",
    "    train_loss+=loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "  avg_train_loss=train_loss/len(train_dataloader)\n",
    "  print(\"Loss For This Epoch is {}\".format(avg_train_loss))\n",
    "  model.eval()\n",
    "  val_loss=0\n",
    "  val_accuracy=0\n",
    "\n",
    "  for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    with torch.no_grad():        \n",
    "      (loss, logits) = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "    val_loss += loss.item()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    flag1=[];flag2=[]\n",
    "    for i in label_ids:\n",
    "      flag1.append(i)\n",
    "    z=get_label(logits)\n",
    "    for i in z:\n",
    "      flag2.append(i)  \n",
    "    temp2 = (time.time()-t0)\n",
    "  result=get_accuracy(flag1,flag2)\n",
    "  print(\"-----Running the {} epoch. Time Taken is{}-------\".format(epoch,temp2))\n",
    "  print('---- Accuracy for this Epoch is {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBzQLrjqG5zV"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "sentences = test_tweets\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                         sent,   \n",
    "                        truncation=True,\n",
    "                                          # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aOaNwj5lG_EJ",
    "outputId": "1a8e0f13-b0a1-4589-a06e-d6f33361b8f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,000 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  #label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  predictions.append(logits)\n",
    "  #true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhE4OYk2HBMa"
   },
   "outputs": [],
   "source": [
    "pred_labels_i=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9V9eGYR3HD-I"
   },
   "outputs": [],
   "source": [
    "for i in range(len(input_ids)):\n",
    "  pred_labels_i.append(np.argmax(predictions[i], axis=1).flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnim6OM5HFpW"
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ES4_XmhHJDM",
    "outputId": "986a3745-1c2e-4667-833a-7e41355c26f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.00648014848848"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(testlabels['Sentiment'], pred_labels_i, average=\"weighted\")*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0MP8RAoHK1i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ProjectIIITA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07f5bc81f9b64e5983cef30295201dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1327a10be7434cccbc33f4fe0582da28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19a45bf1609b420391e6a1e32663e587": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ec373ea178a4b39bc432b9b50c89b96",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_514bb8ad8ddc4284b4d86ddc5e397d5d",
      "value": 433
     }
    },
    "1d1c5984db67487ea9b36da01d663472": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ec373ea178a4b39bc432b9b50c89b96": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c30f66e8ff476d9295f21fb3988877": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d30092e719f04cda94274dfd0e70819f",
       "IPY_MODEL_a7e865f4494843b29ba73851c948fd6d"
      ],
      "layout": "IPY_MODEL_d02ad7f151c341ed952aaa6597bcfc1e"
     }
    },
    "49256d6694284f53a05a4cf914e8c7b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a75340b61dc41fcb11e64bb1bb3a807": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514bb8ad8ddc4284b4d86ddc5e397d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d77f362f6354298a8585b6ffe18af45": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ba2ade06e6745babc9308504dcdf06d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a75340b61dc41fcb11e64bb1bb3a807",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1d1c5984db67487ea9b36da01d663472",
      "value": " 232k/232k [00:00&lt;00:00, 260kB/s]"
     }
    },
    "94f3087e0e614ae281fe6bea3b580bf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7e865f4494843b29ba73851c948fd6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8b3ad3c079c4c95be8031bf156a8f60",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_94f3087e0e614ae281fe6bea3b580bf1",
      "value": " 440M/440M [00:06&lt;00:00, 71.0MB/s]"
     }
    },
    "b22b466a66a84297b75109c83713f05f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1c354adca47412796056c5ded6b3367",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c11e6bb7c9ee4dc383a06220b27370d8",
      "value": 231508
     }
    },
    "b50648dd6dff4edea1d15b75fc302725": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19a45bf1609b420391e6a1e32663e587",
       "IPY_MODEL_d5138f970ac24380aaf7a78906b9c60d"
      ],
      "layout": "IPY_MODEL_ea6abe68768b40d0b5959372c7431f5c"
     }
    },
    "b8b3ad3c079c4c95be8031bf156a8f60": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c11e6bb7c9ee4dc383a06220b27370d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c1c354adca47412796056c5ded6b3367": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3ae0d87308542e29bdedabb8ae48b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b22b466a66a84297b75109c83713f05f",
       "IPY_MODEL_7ba2ade06e6745babc9308504dcdf06d"
      ],
      "layout": "IPY_MODEL_49256d6694284f53a05a4cf914e8c7b2"
     }
    },
    "d02ad7f151c341ed952aaa6597bcfc1e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d30092e719f04cda94274dfd0e70819f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d77f362f6354298a8585b6ffe18af45",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07f5bc81f9b64e5983cef30295201dec",
      "value": 440473133
     }
    },
    "d5138f970ac24380aaf7a78906b9c60d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1327a10be7434cccbc33f4fe0582da28",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_de77d628a4454e75ab33637c3bbf450b",
      "value": " 433/433 [00:00&lt;00:00, 1.09kB/s]"
     }
    },
    "de77d628a4454e75ab33637c3bbf450b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea6abe68768b40d0b5959372c7431f5c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
